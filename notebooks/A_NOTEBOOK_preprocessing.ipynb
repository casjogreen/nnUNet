{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fa676-34d5-407b-a31a-2aa250bb95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import json, os\n",
    "from pydicom import dcmread\n",
    "import seaborn as sns   \n",
    "import pandas as pd\n",
    "from utilities import natural_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d88b",
   "metadata": {},
   "source": [
    "The cell below creates a log file for the current notebook. This can help with debugging and record keeping. You can find the file in the `logs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "\n",
    "logger  = logging.getLogger(__name__)\n",
    "log_file = os.path.join('logs','preprocessing.log')\n",
    "logging.basicConfig(filename=log_file, filemode='w',\n",
    "                    level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "867dcc4e-d72b-49eb-b773-44355e9b33d9",
   "metadata": {},
   "source": [
    "The Json file containing the user preferences is central to all of the classes used in this demo. The directory of this file is used as an input when the classes are instantialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61807a-ba88-4de6-b1cc-6a194e3a8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs_dir = \"configuration_files/user_config.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ac4e1d-86d6-43ce-a1d5-a1217ff0402e",
   "metadata": {},
   "source": [
    "---\n",
    "## The DicomToolbox package\n",
    "\n",
    "This is a class designed for parsing DICOM files. Unlike previous iterations, this version does not rely on HDF5 files to minimize the RAM load. The HDF5 dependence served well when we were using this class to simulate perturbed dose distributions, which used as many as 23 processors and was memory demanding. We intended for this version of the DicomToolbox to be used in serial applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicom_toolbox import DicomToolbox\n",
    "\n",
    "dt = DicomToolbox(user_inputs_dir)\n",
    "# dt.expected_data = ['ct', 'rtdose', 'rtstruct'] # allows you to process patients with only these data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66bb12",
   "metadata": {},
   "source": [
    "**Identifying patients folders with the necessary files**\n",
    "\n",
    "The cell below will identify all the patients folders that contain the necessary files for dose prediction with this tool. The method searches for folders inside the folder you specify in the `raw_patient_data` field of the `user_config` JSON file.\n",
    "\n",
    "\n",
    "Note: If you want to use the `DicomToolbox` class to read data with missing plan files, you can remove `rtplan` from the value of `self.expected_data`. This should be rare, so you can ignore this note if it is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids_found = dt.identify_patient_files()\n",
    "\n",
    "print(f'No. of patients found: {len(patient_ids_found)}')\n",
    "print(f'Patients found: {\", \".join(sorted(patient_ids_found))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45b97f",
   "metadata": {},
   "source": [
    "### NEW: You can display the content of the header of the DICOM files with the `get_header_info` method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.get_header_info(1, 'ct', save_to_file=True, echo=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d318920-a748-4000-86d8-cd7b031477e5",
   "metadata": {},
   "source": [
    "### Parsing the DICOM files of a single patient\n",
    "\n",
    "In this example, the resolution of the masks or structures will be set to match the resolution of the CT volume. This is done by setting the `mask_resolution` input variable to `\"ct\"` when calling the `parse_dicom_files()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e4f92-9b47-47f0-8489-3ac078fed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ID = 1\n",
    "\n",
    "# the next line is optional and I include it just for demonstration purposes\n",
    "# dt.desired_contour_set ='clinical' # in some cases, you may have auto-contours, which can be read by setting this to 'auto' \n",
    "\n",
    "dt.parse_dicom_files(patient_ID, mask_resolution='ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45599f6f",
   "metadata": {},
   "source": [
    "When you parse data for a patient, the structures/contours/masks are stored in a dictionary called `self.contours`. You can view the content of the dictionary by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in dt.contours.keys():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.cumulative_dose.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = 80\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(dt.ct.data[sn,:,:], cmap='gray', vmin=-100, vmax = 300)\n",
    "for c in dt.contours.keys():\n",
    "    try:\n",
    "        plt.contour(dt.contours[c].data[sn,:,:], colors=[np.random.rand(3,)])\n",
    "    except:\n",
    "        pass\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74641ec",
   "metadata": {},
   "source": [
    "Getting the cummulative dose is easy with the DicomToolbox class. You can use the method `cumulative_dose`, which is a class property, to get the cummulative dose. Since it is a class property, you do not need to use parenthesis to call it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8380118",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dose = dt.cumulative_dose\n",
    "\n",
    "print(f\"Mean dose: {np.mean(cum_dose):.2f} Gy\")\n",
    "print(f\"Max dose: {np.max(cum_dose):.2f} Gy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = 73\n",
    "fig, ax = plt.subplots(figsize=(7,7)) # create subplot\n",
    "im = ax.imshow(cum_dose[sn,:,:], cmap='jet')\n",
    "\n",
    "divider = make_axes_locatable(ax) # TIP: this is used to make the colorbar the same height as the image\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax) # add colorbar to subplot\n",
    "cbar.ax.set_ylabel('Cumulative Dose (Gy)', labelpad=10)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ad675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.radiation_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa4a53",
   "metadata": {},
   "source": [
    "Note for Josiane: Change the number below to show only the dose above the value you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b964fb8",
   "metadata": {},
   "source": [
    "Another option when you parse the data, which is helpful when you are inspecting a dataset and do not want to build all of the masks, is to set `mask_name_only` to `True`. In this case, the contours will not be returned as a dictionary but rather as a list of the names of the contours. This is demonstrated in the cell below.\n",
    "\n",
    "Note that this execution will take only a few seconds at the expense of less data. You still have the CT and dose volumes as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.parse_dicom_files(patient_ID, mask_names_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d71cbf",
   "metadata": {},
   "source": [
    "In the previous example, we parsed the contours at the resolution of the CT. Now we can check the option for parsing at the resolution of the dose. This option is useful for tasks like computing a DVH, which we can perform in the cells below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.parse_dicom_files(patient_ID, mask_resolution='dose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydicom import dcmread\n",
    "from pydicom.sequence import Sequence\n",
    "from pydicom.dataset import Dataset\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from skimage.draw import polygon\n",
    "from dataclasses import dataclass, field\n",
    "from utilities import interpolate_volume, natural_keys\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import find_contours\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import os, json, copy, pandas, h5py, logging, re, pickle, pydicom, traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class Coordinates(): \n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    z: np.ndarray\n",
    "    dx: float\n",
    "    dy: float\n",
    "    dz: float\n",
    "    image_position: np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class CT():\n",
    "    shape: tuple  \n",
    "    resolution: np.ndarray\n",
    "    max_value: float\n",
    "    min_value: float  \n",
    "    units: str \n",
    "    rescale_slope: float\n",
    "    rescale_intercept: float\n",
    "    patient_position: str\n",
    "    data: np.ndarray\n",
    "    slice_thickness: float\n",
    "    coordinates: Coordinates\n",
    "    \n",
    "@dataclass\n",
    "class CommonDoseTags():\n",
    "    shape: tuple\n",
    "    max_value: float\n",
    "    min_value: float\n",
    "    resolution: np.ndarray\n",
    "    dose_grid_scaling: float\n",
    "    dose_units: str\n",
    "    data: np.ndarray\n",
    "    coordinates: Coordinates\n",
    "    beam_number: int = 1\n",
    "    beam_type: str = 'not_specified'\n",
    "    gantry_angle: float = 0.0\n",
    "    patient_support_angle: float = 0.0\n",
    "    table_top_pitch_angle: float = 0.0\n",
    "    table_top_roll_angle: float = 0.0\n",
    "    isocenter: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    radiation_type: str = 'not_specified'\n",
    "    treatment_delivery_type: str = 'not_specified'\n",
    "    beam_name: str = 'not_specified'\n",
    "    treatment_machine: str = 'not_specified'\n",
    "    beam_description: str = 'not_specified'\n",
    "    number_of_control_points: int = 0\n",
    "    final_cumulative_meterset_weight: float = 0.0\n",
    "    beam_dose: float = 0.0\n",
    "    scan_mode: str = 'not_specified'\n",
    "    primary_dosimetric_units: str = 'not_specified'\n",
    "     \n",
    "@dataclass\n",
    "class ProtonDose(CommonDoseTags):\n",
    "    vsad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    \n",
    "@dataclass\n",
    "class PhotonDose(CommonDoseTags):\n",
    "    sad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    gantry_rotation_direction: list = field(default_factory=lambda: ['not_specified'])\n",
    "    \n",
    "@dataclass\n",
    "class Mask():\n",
    "    data: np.ndarray\n",
    "    resolution: str\n",
    "    coordinates:Coordinates\n",
    "    number: int = 0\n",
    "    structure_set: str = 'clinical'\n",
    "    generation_method: str = 'unknown'\n",
    "    \n",
    "@dataclass\n",
    "class Plan():\n",
    "    number_of_beams: int = 1\n",
    "    geometry: str = 'not_specified'\n",
    "    patient_position: str = 'not_specified'\n",
    "    patient_sex: str = 'not_specified'\n",
    "    plan_label: str = 'not_specified'\n",
    "    number_of_fractions_planned: int = 0\n",
    "    dose_per_fraction: float = 0.0\n",
    "    dose_reference_type: str = 'not_specified'\n",
    "    dose_reference_description: str = 'not_specified'\n",
    "    dose_reference_dose: float = 0.0\n",
    "    radiation_type: str = 'not_specified'\n",
    "    beam: dict = field(default_factory=lambda: {})\n",
    "    \n",
    "@dataclass\n",
    "class Beam():\n",
    "    gantry_angle: float = 0.0\n",
    "    patient_support_angle: float = 0.0\n",
    "    isocenter: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    treatment_delivery_type: str = 'not_specified'\n",
    "    treatment_machine: str = 'not_specified'\n",
    "    type: str = 'not_specified'\n",
    "    sad: float = 0.0\n",
    "    vsad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    \n",
    "class DicomToolbox():\n",
    "    \"\"\"\"Class for parsing a set of DICOM-RT files for a patient.\n",
    "\n",
    "        Created on the Fall of 2021 by Ivan Vazquez in collaboration with Ming Yang. \n",
    "\n",
    "        Last updated: January 2024\n",
    "\n",
    "        Copyright 2021-2024 Ivan Vazquez\n",
    "    \"\"\"\n",
    "    original_ct_coordinates =Coordinates(0, 0, 0, 0, 0, 0, [0, 0, 0])\n",
    "    original_dose_coordinates = Coordinates(0, 0, 0, 0, 0, 0, [0, 0, 0])\n",
    "    \n",
    "    def __init__(self, user_inputs_dir=None, patient_data_directory=None, lut_directory=None) -> None:\n",
    "        \n",
    "        self.__logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Prepare RSP LUT directory\n",
    "        if lut_directory is not None:\n",
    "            lut_directory = os.path.join('utilities','LUT')\n",
    "            if not os.path.isdir(lut_directory): \n",
    "                self.__logger.warning(f'The directory {lut_directory} for the look-up tables does not exist.')\n",
    "                self.lut_directory = None\n",
    "            else:\n",
    "                self.lut_directory = lut_directory\n",
    "\n",
    "        # Load user input information if a directory is provided\n",
    "        if user_inputs_dir is not None:\n",
    "            with open(user_inputs_dir, \"r\") as f:\n",
    "                self.user_inputs = json.load(f)  \n",
    "                self.patient_data_directory = self.user_inputs['DIRECTORIES']['raw_patient_data']\n",
    "        else:\n",
    "            self.__set_default_user_inputs()\n",
    "            \n",
    "        # set the directory for the patient data to the specified value if one is given\n",
    "        if patient_data_directory is not None: self.patient_data_directory = patient_data_directory\n",
    "        assert self.patient_data_directory is not None\n",
    "\n",
    "        # Check if the necessary directories exists\n",
    "        for directory in ['logs', 'temp', os.path.join('temp','data')]:\n",
    "            if not os.path.isdir(directory): os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "        # Initialize variables\n",
    "        self.reset()\n",
    "        self.parallelize = None\n",
    "        self.n_threads = self.user_inputs[\"PARALLELIZATION\"][\"number_of_processors\"]\n",
    "        self.min_coordinate_precision = 3\n",
    "    \n",
    "    def __set_default_user_inputs(self):\n",
    "            \n",
    "            self.user_inputs = {\n",
    "                \"TYPE_OF_TARGET_VOLUME\": \"ctv\",\n",
    "                \"PARALLELIZATION\": {\n",
    "                    \"number_of_processors\": 1,\n",
    "                },\n",
    "                \"DATA_PREPROCESSING\": {\n",
    "                    \"contour_interpolation_method\":\"nearest\",\n",
    "                }\n",
    "            }\n",
    "            self.patient_data_directory = None\n",
    "            self.__logger.warning(\"No user inputs were provided. Setting default values.\")\n",
    "            \n",
    "    def reset(self):\n",
    "        \n",
    "        self.patient_id = None\n",
    "        self.original_ct_coordinates = None\n",
    "        self.original_dose_coordinates = None\n",
    "        self.mask_interpolation_technique = self.user_inputs[\"DATA_PREPROCESSING\"][\"contour_interpolation_method\"]\n",
    "        self.mask_generation_method = 'interpolate'\n",
    "        self.write_new_hdf5_file = True\n",
    "        self.relevant_masks = None\n",
    "        self.compression = 'lzf'\n",
    "        self.expected_data = ['ct', 'rtdose', 'rtplan', 'rtstruct']\n",
    "        self.radiation_type = None\n",
    "        self.echo_progress = True\n",
    "        self.echo_level = 0\n",
    "        self.coordinate_precision = 3\n",
    "        self.equalize_dose_grid_dimensions = True\n",
    "          \n",
    "    def identify_patient_files(self, patient_data_directory = None, echo=False):\n",
    "        \"\"\"Function to identify the number of patient folders with all DICOM-RT files \n",
    "           needed for proper functioning of the code.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "\n",
    "            `patient_data_directory` : str\n",
    "                The location of the patient data folders containing the required DICOM-RT files.\n",
    "\n",
    "            `echo` : bool\n",
    "                Flag to prompt funtion to write the number of patient folders found\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "\n",
    "            `list`\n",
    "                Names of folders for the patients found.\n",
    "        \"\"\"\n",
    "\n",
    "        # check if a directory was specified\n",
    "        if patient_data_directory is not None: self.patient_folders_directory = patient_data_directory\n",
    "\n",
    "        # get a list of all of the folders in the directory\n",
    "        folders = os.listdir(self.patient_data_directory)\n",
    "\n",
    "        # check folder content to avoid future errors\n",
    "        patient_files_info = {f:{'modalities':[], 'folder_directory':''} for f in folders}\n",
    "        \n",
    "        self.__logger.info(f'Checking the content of {self.patient_data_directory} to identify patient folders with the required DICOM-RT files.')\n",
    "        \n",
    "        for folder in folders:\n",
    "\n",
    "            patient_folder_directory = os.path.join(self.patient_data_directory, folder)\n",
    "            \n",
    "            for root, _, files in os.walk(patient_folder_directory):\n",
    "                \n",
    "                for file in files:\n",
    "        \n",
    "                    # grab modality for DICOM file\n",
    "                    file_directory = os.path.join(root, file)\n",
    "                    \n",
    "                    try:\n",
    "                        ds = dcmread(file_directory)\n",
    "\n",
    "                        modality = ds.data_element('Modality').value\n",
    "                                                \n",
    "                        patient_files_info[folder]['modalities'].append(modality.lower())\n",
    "                                                \n",
    "                    except:\n",
    "                        self.__logger.warning(f\"The content for the folder '{folder}' could not be read\")\n",
    "                        break\n",
    "            \n",
    "            # remove repeated modality values and sort the resulting list                                        \n",
    "            patient_files_info[folder]['modalities'] = sorted(list(set(patient_files_info[folder]['modalities'])))\n",
    "            patient_files_info[folder]['folder_directory'] = patient_folder_directory\n",
    "         \n",
    "        # Record the data folders with the required DICOM-RT files\n",
    "        patients = []\n",
    "        for p in patient_files_info.keys():\n",
    "            if not all(m in patient_files_info[p]['modalities'] for m in self.expected_data):\n",
    "                self.__logger.warning(f\"The folder '{p}' is missing one or more of the required DICOM-RT files. \"\n",
    "                                      f\"Current modalities: {', '.join(patient_files_info[p]['modalities'])}\")                           \n",
    "            else:\n",
    "                patients.append(p)\n",
    "        \n",
    "        patients.sort(key=natural_keys)\n",
    "        \n",
    "        if echo: self.__logger.info(f'Found {len(patients)} patient folders in {patient_data_directory} with the required DICOM-RT files.')\n",
    "\n",
    "        return patients\n",
    "    \n",
    "    def get_header_info(self, patient_id, file_type, save_to_file=False, echo=False):\n",
    "          \n",
    "        patient_files = self.run_initial_check(patient_id)\n",
    "        \n",
    "        if file_type == 'ct':\n",
    "            files = patient_files['ct']\n",
    "        elif file_type == 'dose':\n",
    "            files = patient_files['dose']\n",
    "        elif file_type == 'plan':\n",
    "            files = patient_files['plan']\n",
    "        elif file_type == 'structures':\n",
    "            files = patient_files['structures']\n",
    "        else:\n",
    "            self.__logger.error(f'Invalid file type {file_type}.')\n",
    "            return\n",
    "        \n",
    "        for n, f in enumerate(files):\n",
    "            ds = dcmread(f)    \n",
    "            \n",
    "            if echo: print(ds) # print the header information\n",
    "            \n",
    "            # save pretty json to file in log directory\n",
    "            if save_to_file:\n",
    "                header_output_dir = os.path.join('logs', f'{file_type}_header_info_{file_type}_{n}.json')\n",
    "                with open(header_output_dir, 'w') as outfile:\n",
    "                    print(ds, file=outfile)\n",
    "            \n",
    "            # if CT, exit\n",
    "            if file_type == 'ct': break\n",
    "                        \n",
    "    def identify_radiation_type(self, patient_files, patient_id):\n",
    "        \"\"\"Function to identify the main type of radiation therapy used for a patient. The \n",
    "        function determines the most common radiation type used for the beams in the plan file.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ds = dcmread(patient_files['plan'][0])\n",
    "                \n",
    "        if self.radiation_type is not None: return\n",
    "                        \n",
    "        try:            \n",
    "            radiation_types = [b.RadiationType.lower() for b in ds.BeamSequence]\n",
    "            unique_radiation_types = list(set(radiation_types))\n",
    "            self.radiation_type = max(unique_radiation_types, key=radiation_types.count) \n",
    "        except:\n",
    "            try:\n",
    "                radiation_types = [b.RadiationType.lower() for b in ds.IonBeamSequence]\n",
    "                unique_radiation_types = list(set(radiation_types))\n",
    "                self.radiation_type = max(unique_radiation_types, key=radiation_types.count)                \n",
    "            except:\n",
    "                self.__logger.error(f'Failed to identify the radiation type for pat-{patient_id}.')\n",
    "                self.__logger.info('If you know the radiation type, please specify it with the class attribute \"radiation_type\".')   \n",
    "                                                        \n",
    "    def run_initial_check(self, patient_id=None):\n",
    "        \n",
    "        assert self.patient_data_directory is not None\n",
    "        if patient_id is not None:\n",
    "            if type(patient_id) != type(\"\"): patient_id = str(patient_id)\n",
    "            self.patient_id = patient_id\n",
    "        \n",
    "        # prepare patient data directory\n",
    "        patient_directory = os.path.join(self.patient_data_directory, self.patient_id)\n",
    "        \n",
    "        # Detect all files in the directory\n",
    "        try:\n",
    "            files = os.listdir(patient_directory)\n",
    "        \n",
    "            # Find directory of all type of DICOM files \n",
    "            patient_files = {'ct':[], 'plan':[], 'structures':[], 'dose':[]}\n",
    "            \n",
    "            # Discover all of the files for the patients\n",
    "            for root, _, files in os.walk(patient_directory):\n",
    "                for f in files:\n",
    "                    # Get modality for the file\n",
    "                    ds = dcmread(os.path.join(root,f))\n",
    "                    modality = ds.data_element('Modality').value.lower()\n",
    "                    \n",
    "                    # Add file to the corresponding list\n",
    "                    if modality == 'rtdose':\n",
    "                        patient_files['dose'].append(os.path.join(root,f))\n",
    "        \n",
    "                    elif modality == 'rtplan':\n",
    "                        patient_files['plan'].append(os.path.join(root,f))\n",
    "                        \n",
    "                    elif modality == 'rtstruct':\n",
    "                        patient_files['structures'].append(os.path.join(root,f)) \n",
    "                        patient_files['structures'] = sorted(patient_files['structures'])\n",
    "                        \n",
    "                    elif modality == 'ct':\n",
    "                        patient_files['ct'].append(os.path.join(root,f))     \n",
    "                      \n",
    "        except Exception as e:\n",
    "            self.__logger.error(f\"An error occured while trying to read the files for patient {self.patient_id}.\")\n",
    "            self.__logger.error(traceback.format_exc())\n",
    "            return None\n",
    "        \n",
    "        print(patient_files['ct'])  \n",
    "        \n",
    "        # identify the radiation type\n",
    "        if self.radiation_type is None: self.identify_radiation_type(patient_files, self.patient_id)\n",
    "    \n",
    "        # Detect incomplete data\n",
    "        file_types_dict = {'ct':'ct', 'rtdose':'dose', 'rtplan':'plan', 'rtstruct':'structures'}\n",
    "        if any([patient_files[k]==[] for k in [file_types_dict[x] for x in self.expected_data]]):\n",
    "            self.__logger.error(f'The full DICOM-RT set ({\", \".join(self.expected_data)}) for patient {self.patient_id} could not be read.')\n",
    "            self.__logger.info(\"Please change the expected data types by specifying the class attribute 'expected_data' or check the patient folder.\")\n",
    "            return None\n",
    "        \n",
    "        # Check the dose files to check for beam-specific or cumulative dose        \n",
    "        patient_files['dose'] = self.__check_dose_files(patient_files['dose'])\n",
    "            \n",
    "        return patient_files\n",
    "    \n",
    "    def __check_dose_files(self, dose_files):\n",
    "                \n",
    "        dose_file_info = {n:{} for n in dose_files}\n",
    "        \n",
    "        for f in dose_files:\n",
    "            with dcmread(f) as ds:\n",
    "                dose_file_info[f]['dose_summation_type'] = ds.DoseSummationType.lower()\n",
    "                dose_file_info[f]['data'] = ds.pixel_array * ds.DoseGridScaling\n",
    "                try:\n",
    "                    dose_file_info[f]['beam_number'] = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                except:\n",
    "                    dose_file_info[f]['beam_number'] = 'not_specified'\n",
    "        \n",
    "        # check the dose summation type\n",
    "        if len(set([dose_file_info[f]['dose_summation_type'] for f in dose_files])) > 1:\n",
    "            self.__logger.info(f'Multiple dose summation types were identified for patient {self.patient_id}: {\", \".join([dose_file_info[f][\"dose_summation_type\"] for f in dose_files])}')\n",
    "\n",
    "            # check total dose \n",
    "            cum_dose = np.sum([dose_file_info[f]['data'] for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'beam'], axis=0)\n",
    "            # grab the dose data for file with plan as dose summation type\n",
    "            plan_dose = [dose_file_info[f]['data'] for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'plan'][0]\n",
    "            \n",
    "            if np.round(np.abs(np.subtract(cum_dose,plan_dose).max())) > 0.0:\n",
    "                self.__logger.error(f'The sum of the beam dose for patient {self.patient_id} does not match the dose for the plan file.')\n",
    "                raise ValueError()\n",
    "            else:\n",
    "                return [f for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'beam']\n",
    "            \n",
    "        else:\n",
    "            # return the dose files\n",
    "            return dose_files\n",
    "                        \n",
    "    def parse_dicom_files(self, patient_id=None, parse_structures=True, mask_resolution = None, patient_data_directory =None):\n",
    "        \n",
    "        # import tools for timing execution\n",
    "        from time import time\n",
    "        \n",
    "        # check time for setting things up\n",
    "        start = time()\n",
    "        if patient_data_directory is not None: \n",
    "            self.user_inputs['DIRECTORIES']['raw_patient_data'] = patient_data_directory\n",
    "        else:\n",
    "            patient_data_directory  = self.user_inputs['DIRECTORIES']['raw_patient_data']\n",
    "        \n",
    "        if patient_data_directory is None:\n",
    "            self.__logger.error('No patient data directory was specified.')\n",
    "            raise Exception('Check log file')\n",
    "        \n",
    "        # initial checks\n",
    "        if patient_id is not None: \n",
    "            if type(patient_id) != type(''): patient_id = str(patient_id)\n",
    "            self.patient_id = patient_id\n",
    "        if patient_id is None and self.patient_id is None:\n",
    "            self.__logger.error('Calling DICOM parsing function without specifying a `patient_id`.')\n",
    "            raise Exception('Check log file')\n",
    "        self.mask_resolution = mask_resolution if mask_resolution is not None else 'dose'\n",
    "        \n",
    "        if patient_data_directory  is not None and patient_data_directory .split('.')[-1] in ['h5', 'hdf5']:\n",
    "            self.read_data_from_hdf5()\n",
    "            return\n",
    "            \n",
    "        # Identify the file types in the patient folder and type of radiation therapy\n",
    "        self.dicom_files = self.run_initial_check(self.patient_id)\n",
    "        \n",
    "        # report time \n",
    "        print(f'Time to set up: {time()-start:.2f} seconds')\n",
    "                        \n",
    "        \n",
    "        # time to parse the files\n",
    "        start = time()\n",
    "                        \n",
    "        # Parse the CT volume\n",
    "        self.ct = self.parse_ct_study_files(self.dicom_files['ct'])\n",
    "        \n",
    "        print(f'Time to parse CT: {time()-start:.2f} seconds')\n",
    "\n",
    "        start = time()\n",
    "\n",
    "        # Parse the dose volume and (optionally) the plan \n",
    "        if 'plan' in self.dicom_files.keys() and self.dicom_files['plan'] != []:\n",
    "            self.dose, self.plan = self.parse_rt_dose_files(self.dicom_files['dose'], self.dicom_files['plan'])\n",
    "        else:\n",
    "            self.__logger.warning(f'No plan file was found for patient {self.patient_id}. Using default values for the plan.')\n",
    "            self.dose = self.parse_rt_dose_files(self.dicom_files['dose'])\n",
    "            self.plan = Plan()\n",
    "            \n",
    "        print(f'Time to parse dose: {time()-start:.2f} seconds')\n",
    "        \n",
    "        start = time()\n",
    "                                                     \n",
    "        # Parse the contours\n",
    "        self.contours = self.parse_structure_files(sorted(self.dicom_files['structures']), names_only=not parse_structures, resolution=self.mask_resolution)\n",
    "        \n",
    "        print(f'Time to parse structures: {time()-start:.2f} seconds')\n",
    "\n",
    "    def parse_ct_study_files(self, files=None, patient_id = None, units='hu'):\n",
    "      \n",
    "        if patient_id is not None: \n",
    "            self.patient_id = str(patient_id)\n",
    "            files = self.run_initial_check(self.patient_id)['ct']\n",
    "            \n",
    "        # Prepare CT volume\n",
    "        ct_slices = {dcmread(f).ImagePositionPatient[-1]:dcmread(f).pixel_array for f in files}\n",
    "        ## Construct the z coordinate array \n",
    "        z = sorted(list(ct_slices.keys()))\n",
    "        ## Build 3D CT dataset\n",
    "        data = np.array([ct_slices[i].astype(float) for i in z])\n",
    "        ## Determine the number of slice spacings used for the CT data         \n",
    "        z_spacing = list(set(list(np.round(np.array(z[1:]) - np.array(z[0:-1]), self.coordinate_precision))))\n",
    "        z = np.round(z, self.coordinate_precision)\n",
    "        \n",
    "        with dcmread(files[0]) as ds:\n",
    "            \n",
    "            # Grab the position of the patient\n",
    "            patient_position = ds.PatientPosition.lower()\n",
    "                   \n",
    "            ## Grab image position (patient) attribute\n",
    "            image_position = [np.round(p, self.coordinate_precision) for p in ds.ImagePositionPatient]\n",
    "                       \n",
    "            ## Update z-value of image position\n",
    "            image_position[-1] = z[0]\n",
    "\n",
    "            ## Store CT Study information\n",
    "            xy_resolution = [np.round(float(i),self.coordinate_precision) for i in ds.PixelSpacing]\n",
    "            rescale_slope = ds.RescaleSlope\n",
    "            rescale_intercept = ds.RescaleIntercept     \n",
    "            slice_thickness = np.round(ds.SliceThickness, self.coordinate_precision)\n",
    "            \n",
    "            # if np.round(ds.SliceThickness,self.coordinate_precision) not in z_spacing:\n",
    "            #     self.__logger.warning(f'Mistmatch between the slice thickness {ds.SliceThickness}-mm and '\n",
    "            #                           f'coordinate spacings ({\", \".join([str(i) for i in z_spacing])})-mm for patient {self.patient_id}.')\n",
    "                 \n",
    "        ## Prepare the x and y coordinates\n",
    "        x = np.round(np.arange(data.shape[2]) * xy_resolution[0] + image_position[0], self.coordinate_precision)\n",
    "        y = np.round(np.arange(data.shape[1]) * xy_resolution[1] + image_position[1], self.coordinate_precision)\n",
    "                 \n",
    "        ## Interpolate volume if multiple slice thicknesses were used\n",
    "        if len(z_spacing) > 1:\n",
    "            min_dz = np.round(min(z_spacing), self.coordinate_precision)\n",
    "            min_z, max_z = np.round(np.min(z), self.coordinate_precision), np.round(np.max(z), self.coordinate_precision)\n",
    "            self.__logger.warning(f\"Multiple slice thicknesses were identified for the CT data of patient {self.patient_id}: {', '.join([str(i) for i in z_spacing])} mm\")  \n",
    "            self.__logger.info(f'Interpolating CT data to achieve a uniform slice thickness of {min_dz}-mm')\n",
    "            ### define new z-coordinates\n",
    "            z_new = np.round(np.arange(min_z, max_z, min_dz), self.coordinate_precision)\n",
    "            original_coordinates = (z, y, x)\n",
    "            interpolation_coordinates = (z_new, y, x) \n",
    "            data = interpolate_volume(data, original_coordinates, interpolation_coordinates, \n",
    "                                      intMethod='linear', boundError=False, fillValue=0)\n",
    "            z = z_new\n",
    "        \n",
    "        ## Create coordinate object for CT data\n",
    "        dx, dy, dz = np.array(xy_resolution + [np.round(min(z_spacing), self.coordinate_precision)])\n",
    "        coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "\n",
    "        ## Convert units to HU if specified\n",
    "        if units == 'hu': data = data * rescale_slope + rescale_intercept\n",
    "        if units != 'hu': units = 'original'    \n",
    "\n",
    "        ## Save a copy of the original CT information to help create the masks\n",
    "        self.original_ct_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "        self.original_ct_shape = data.shape\n",
    "\n",
    "        return CT(data.shape, (dx,dy,dz), np.max(data), np.min(data), units, rescale_slope,\n",
    "                  rescale_intercept, patient_position, data, slice_thickness, coordinates)\n",
    "    \n",
    "    def get_plan_info(self, dsP):\n",
    "                    \n",
    "        plan = Plan()\n",
    "\n",
    "        plan.geometry = dsP.RTPlanGeometry.lower() if hasattr(dsP, \"RTPlanGeometry\") else 'not_specified'\n",
    "        plan.patient_sex = dsP.PatientSex.lower() if dsP.PatientSex != '' else 'not_specified'\n",
    "        plan.radiation_type = self.radiation_type.lower()\n",
    "        \n",
    "        for fgs in dsP.FractionGroupSequence:\n",
    "            plan.number_of_fractions_planned = int(fgs.NumberOfFractionsPlanned) if hasattr(fgs, \"NumberOfFractionsPlanned\") else 0\n",
    "            plan.number_of_beams = int(fgs.NumberOfBeams)\n",
    "            dose_per_beam = []\n",
    "            for rbs in fgs.ReferencedBeamSequence:\n",
    "                dose_per_beam.append(float(rbs.BeamDose) if hasattr(rbs, \"BeamDose\") else 0.0)\n",
    "                \n",
    "            plan.dose_per_fraction = np.sum(dose_per_beam) \n",
    "\n",
    "            if hasattr(fgs, \"DoseReferenceSequence\"):\n",
    "                for drs in fgs.DoseReferenceSequence:\n",
    "                    if hasattr(drs, \"DoseReferenceStructureType\") and drs.DoseReferenceStructureType.lower() == 'site':\n",
    "                        plan.dose_reference_type = drs.DoseReferenceType.lower() if hasattr(drs, \"DoseReferenceType\") else 'not_specified'\n",
    "                        plan.dose_reference_description = drs.DoseReferenceDescription.lower() if hasattr(drs, \"DoseReferenceDescription\") else 'not_specified'\n",
    "                        plan.dose_reference_dose = drs.TargetPrescriptionDose.lower() if hasattr(drs, \"TargetPrescriptionDose\") else 0.0\n",
    "            \n",
    "            plan.plan_label = dsP.RTPlanLabel.lower() if hasattr(dsP, \"RTPlanLabel\") else 'not_specified'\n",
    "\n",
    "            patient_position = list(set([x.PatientPosition for x in dsP.PatientSetupSequence]))\n",
    "            if len(patient_position) > 1:\n",
    "                self.__logger.warning(f'Multiple patient positions were identified for patient {self.patient_id}: {\", \".join(patient_position)}')\n",
    "            else:\n",
    "                plan.patient_position = patient_position[0].lower()\n",
    "        \n",
    "        information_sequence = dsP.IonBeamSequence if self.radiation_type == 'proton' else dsP.BeamSequence\n",
    "        \n",
    "        for b in information_sequence:\n",
    "            cps = b.IonControlPointSequence[0] if self.radiation_type == 'proton' else b.ControlPointSequence[0]\n",
    "            if b.TreatmentDeliveryType.lower() == 'setup': continue # skip setup beams\n",
    "            \n",
    "            plan.beam[int(b.BeamNumber)] = Beam()\n",
    "            plan.beam[int(b.BeamNumber)].type = b.BeamType.lower()\n",
    "            if hasattr(b, \"VirtualSourceAxisDistances\"):\n",
    "                plan.beam[int(b.BeamNumber)].sad = b.VirtualSourceAxisDistances\n",
    "            else: \n",
    "                plan.beam[int(b.BeamNumber)].sad = float(b.SourceAxisDistance)\n",
    "                \n",
    "            plan.beam[int(b.BeamNumber)].gantry_angle = cps.GantryAngle\n",
    "            plan.beam[int(b.BeamNumber)].patient_support_angle = cps.PatientSupportAngle\n",
    "            plan.beam[int(b.BeamNumber)].isocenter = cps.IsocenterPosition\n",
    "            plan.beam[int(b.BeamNumber)].treatment_delivery_type = b.TreatmentDeliveryType.lower()\n",
    "            plan.beam[int(b.BeamNumber)].treatment_machine = b.TreatmentMachineName.lower()\n",
    "                    \n",
    "        return plan\n",
    "    \n",
    "    def get_additional_details_from_plan_file(self, dsP, dose, bn):\n",
    "        \n",
    "        # TODO: VMAT plans have more than one angle. This needs to be handled.\n",
    "                \n",
    "        # grab the beam dose      \n",
    "        for fgs in dsP.FractionGroupSequence:\n",
    "            for rbs in fgs.ReferencedBeamSequence:\n",
    "                if int(rbs.ReferencedBeamNumber) == bn: \n",
    "                    dose.beam_dose = rbs.BeamDose if hasattr(rbs, \"BeamDose\") else 0\n",
    "    \n",
    "        information_sequence = dsP.IonBeamSequence if self.radiation_type == 'proton' else dsP.BeamSequence\n",
    " \n",
    "        for b in information_sequence:\n",
    "            if int(b.BeamNumber) == bn:\n",
    "                \n",
    "                cps = b.IonControlPointSequence[0] if self.radiation_type == 'proton' else b.ControlPointSequence[0]\n",
    "                \n",
    "                dose.beam_type = b.BeamType.lower()\n",
    "                dose.radiation_type = b.RadiationType.lower()\n",
    "                dose.beam_name = b.BeamName.lower()\n",
    "                dose.beam_number = int(b.BeamNumber)\n",
    "                dose.beam_description = b.BeamDescription.lower() if hasattr(b, \"BeamDescription\") else 'not_specified'\n",
    "                dose.treatment_machine = b.TreatmentMachineName.lower()\n",
    "                dose.final_cumulative_meterset_weight = b.FinalCumulativeMetersetWeight\n",
    "                dose.scan_mode = b.ScanMode.lower() if hasattr(b, \"ScanMode\") else 'not_specified'\n",
    "                dose.treatment_delivery_type = b.TreatmentDeliveryType.lower()\n",
    "                dose.primary_dosimetric_units = b.PrimaryDosimeterUnit.lower()\n",
    "                dose.number_of_control_points = int(b.NumberOfControlPoints)\n",
    "                dose.gantry_angle = cps.GantryAngle\n",
    "                dose.patient_support_angle = cps.PatientSupportAngle if hasattr(cps, \"PatientSupportAngle\") else 0.0\n",
    "                dose.table_top_pitch_angle = cps.TableTopPitchAngle if hasattr(cps, \"TableTopPitchAngle\") else 0.0\n",
    "                dose.table_top_roll_angle = cps.TableTopRollAngle if hasattr(cps, \"TableTopRollAngle\") else 0.0\n",
    "                dose.isocenter = cps.IsocenterPosition\n",
    "                \n",
    "                if hasattr(b, \"VirtualSourceAxisDistances\"):\n",
    "                    dose.vsad = b.VirtualSourceAxisDistances\n",
    "                else:\n",
    "                    dose.sad = b.SourceAxisDistance\n",
    "                    dose.gantry_rotation_direction = b.GantryRotationDirection.lower() if hasattr(b, \"GantryRotationDirection\") else 'not_specified'\n",
    "            \n",
    "        return dose\n",
    "\n",
    "    def parse_rt_dose_files(self, dose_files=None, plan_file=None, patient_id = None):\n",
    "        \n",
    "        if patient_id is not None: \n",
    "            self.patient_id = str(patient_id)\n",
    "            dose_files = self.run_initial_check(self.patient_id)['dose']\n",
    "            plan_file = self.run_initial_check(self.patient_id)['plan']\n",
    "\n",
    "        assert dose_files is not None or patient_id is not None \n",
    "                        \n",
    "        dose, args = {}, []\n",
    "        for f in dose_files:\n",
    "            with dcmread(f) as ds:\n",
    "                                \n",
    "                if len(dose_files) == 1: # handles the case for just one dose file\n",
    "                    try: # check if the dose file is a beam-specific dose file\n",
    "                        bn = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                    except: # if not, assume it is a cumulative dose file\n",
    "                        bn = 1\n",
    "                    self.__logger.info(f'Only one dose file was found for patient {self.patient_id}.')\n",
    "                    self.__logger.info('Assuming that the dose file contains the cummulative dose for the plan.')\n",
    "                elif ds.DoseSummationType.lower() != 'plan': # handles the case for multiple dose files (beam-specific)\n",
    "                    bn = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                       \n",
    "                # Grab data\n",
    "                data = ds.pixel_array * ds.DoseGridScaling\n",
    "                # Grab some data properties\n",
    "                units = ds.DoseUnits\n",
    "                xy_resolution = [np.round(float(x), self.coordinate_precision) for x in ds.PixelSpacing]\n",
    "                dose_grid_scaling = float(ds.DoseGridScaling)\n",
    "                image_position = [np.round(float(i), self.coordinate_precision) for i in ds.ImagePositionPatient]\n",
    "                grid_offset_vector = np.round(np.array(ds.GridFrameOffsetVector), self.coordinate_precision)\n",
    "                \n",
    "                # Prepare coordinates\n",
    "                x = np.round(np.arange(ds.Columns)*xy_resolution[0] + image_position[0], self.coordinate_precision)\n",
    "                y = np.round(np.arange(ds.Rows)*xy_resolution[1] + image_position[1], self.coordinate_precision)\n",
    "                z = np.round(grid_offset_vector+ image_position[2], self.coordinate_precision)\n",
    "                ## Determine the number of slice spacings used for the dose data         \n",
    "                z_spacing = list(set(list(np.round(np.array(z[1:]) - np.array(z[0:-1]), self.coordinate_precision))))\n",
    "                ## Interpolate volume if multiple slice thicknesses were used\n",
    "                if len(z_spacing) > 1:\n",
    "                    min_dz = min(z_spacing)\n",
    "                    min_z, max_z = np.round(np.min(z), self.coordinate_precision), np.round(np.max(z), self.coordinate_precision)\n",
    "                    self.__logger.warning(f'Two or more slice thicknesses identified for the dose data of patient {self.patient_id}.')    \n",
    "                    self.__logger.info(f'Interpolating dose data to achieve a uniform slice thickness of {min_dz}-mm')\n",
    "                    ### define new z-coordinates\n",
    "                    z_new = np.round(np.arange(min_z, max_z, min_dz), self.coordinate_precision)\n",
    "                    original_coordinates = (z, y, x)\n",
    "                    interpolation_coordinates = (z_new, y, x) \n",
    "                    data = interpolate_volume(data, original_coordinates, interpolation_coordinates, \n",
    "                                              intMethod='linear', boundError=False, fillValue=0)\n",
    "                    z = z_new\n",
    "                \n",
    "                # Grab coordinate information\n",
    "                dx, dy, dz = xy_resolution + [min(z_spacing)]\n",
    "                coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "                \n",
    "                # Prepare dose object\n",
    "                if self.radiation_type == 'photon':\n",
    "                    dose[bn] = PhotonDose(*[data.shape, data.max(), data.min(), (dx,dy,dz), \n",
    "                                            dose_grid_scaling, units, data, coordinates])\n",
    "                else:\n",
    "                    dose[bn] = ProtonDose(*[data.shape, data.max(), data.min(), (dx,dy,dz), \n",
    "                                            dose_grid_scaling, units, data, coordinates])\n",
    "                \n",
    "                # Grab additional information from plan file if available\n",
    "                if plan_file is not None and plan_file != []:\n",
    "                    with dcmread(plan_file[0]) as dsP:                                         \n",
    "                        dose[bn] = self.get_additional_details_from_plan_file(dsP, dose[bn], bn)\n",
    "                                            \n",
    "        if len(set([dose[bn].data.shape for bn in dose.keys()])) > 1:      \n",
    "            self.__logger.warning(f'Not all of the dose volumes have the same shape for patient {self.patient_id}.')\n",
    "            if self.equalize_dose_grid_dimensions:\n",
    "                self.__logger.info('Equalizing dose grid dimensions')\n",
    "                dose = self.__equalize_dose_grid_dimensions(dose)\n",
    "\n",
    "        # Save a copy of the original dose information to help create the masks\n",
    "        assert len(set([dose[bn].data.shape for bn in dose.keys()])) == 1    \n",
    "        self.original_dose_coordinates = copy.deepcopy(dose[bn].coordinates)\n",
    "        self.original_dose_shape = data.shape\n",
    "\n",
    "        # Grab additional information from plan file if available\n",
    "        if plan_file is not None and plan_file != []: \n",
    "            with dcmread(plan_file[0]) as dsP:\n",
    "            \n",
    "                plan = self.get_plan_info(dsP)\n",
    "\n",
    "            return dose, plan\n",
    "        else:\n",
    "            return dose\n",
    "        \n",
    "    def __equalize_dose_grid_dimensions(self, dose):\n",
    "            \n",
    "        max_shape = np.max([dose[bn].data.shape for bn in dose.keys()], axis=0)\n",
    "\n",
    "        bn_to_correct = []\n",
    "\n",
    "        for k in dose.keys():\n",
    "            num_max_dims = []\n",
    "            for n,s in enumerate(dose[k].data.shape):\n",
    "                if s == max_shape[n]: \n",
    "                    num_max_dims.append(k)\n",
    "            if len(num_max_dims) == 3: \n",
    "                max_dim_bn = k\n",
    "                break\n",
    "\n",
    "        if 'max_dim_bn' not in locals(): raise ValueError('Unable to find a beam with the maximum shape along all dimensions.')\n",
    "        \n",
    "        # determine patient(s) needing correction\n",
    "        for k in dose.keys():\n",
    "            for n,s in enumerate(dose[k].data.shape):\n",
    "                if s != max_shape[n]: \n",
    "                    bn_to_correct.append(k)\n",
    "                    \n",
    "        for b in bn_to_correct:\n",
    "            original_coordinates = (dose[b].coordinates.z, dose[b].coordinates.y, dose[b].coordinates.x) \n",
    "            interpolation_coordinates = (dose[max_dim_bn].coordinates.z, dose[max_dim_bn].coordinates.y, dose[max_dim_bn].coordinates.x)   \n",
    "            data = interpolate_volume(dose[b].data, original_coordinates, interpolation_coordinates,\n",
    "                                    intMethod='linear', boundError=0, fillValue=0)\n",
    "            \n",
    "            dose[b].data = data\n",
    "            dose[b].coordinates = copy.deepcopy(dose[max_dim_bn].coordinates)\n",
    "            dose[b].shape = data.shape\n",
    "                        \n",
    "        return dose\n",
    "                       \n",
    "    def parse_structure_files(self, structure_files=None, patient_id=None, structure_names=None, resolution='dose', names_only=False, parallelize=True, n_threads=None):\n",
    "        \n",
    "        assert patient_id is not None or self.patient_id is not None\n",
    "        self.patient_id = patient_id or self.patient_id\n",
    "        structure_files = structure_files or self.run_initial_check(self.patient_id)['structures']\n",
    "\n",
    "        self.all_stucture_names = []\n",
    "        struc_ds = [dcmread(f) for f in structure_files]\n",
    "        for ds in struc_ds:\n",
    "            self.all_stucture_names += [c['name'] for c in self.read_structure(ds)]\n",
    "            \n",
    "        if names_only: return self.all_stucture_names\n",
    "\n",
    "        if self.original_ct_coordinates is None:\n",
    "            self.parse_ct_study_files(patient_id=self.patient_id)\n",
    "\n",
    "        if self.original_dose_coordinates is None:\n",
    "            self.parse_rt_dose_files(patient_id=self.patient_id)\n",
    "\n",
    "        coordinates = copy.deepcopy(self.original_ct_coordinates if resolution.lower() == 'ct' else self.original_dose_coordinates)\n",
    "        \n",
    "        if self.relevant_masks is not None:\n",
    "            assert all(m in self.all_stucture_names for m in self.relevant_masks)\n",
    "            self.all_stucture_names = self.relevant_masks\n",
    "\n",
    "        n_threads = n_threads or mp.cpu_count()//2 if parallelize else 1\n",
    "        structure_names = structure_names or self.all_stucture_names\n",
    "        structure_subsets = np.array_split(structure_names, n_threads) \n",
    "        \n",
    "        with Pool(processes=n_threads) as pool:\n",
    "            args = [(struc_ds, subset.tolist(), resolution, coordinates) for subset in structure_subsets]\n",
    "            results = pool.starmap(self.process_structures_subset, args)\n",
    "        \n",
    "        # Merging dictionaries from all processes\n",
    "        contours = {k: v for d in results for k, v in d.items()}\n",
    "        # else:\n",
    "        #     contours = self.process_structures_subset(structure_files, mask_names, resolution, coordinates)\n",
    "\n",
    "        # Process contours further as required\n",
    "        return contours\n",
    "    \n",
    "    def process_structures_subset(self, \n",
    "                                  struc_ds, \n",
    "                                  subset, \n",
    "                                  resolution, \n",
    "                                  coordinates):\n",
    "\n",
    "        print(os.getpid(), subset)\n",
    "                \n",
    "        contours = {}\n",
    "        \n",
    "        for ds in struc_ds:\n",
    "            structures = self.read_structure(ds)\n",
    "            info = {s['name']: {'number': s['number'], 'generation_algorithm': s['generation_algorithm']} for s in structures}\n",
    "            for s in subset:\n",
    "                if s not in self.all_stucture_names: \n",
    "                    self.__logger.warning(f'A mask \"{s}\" was not found for patient-{self.patient_id}.')\n",
    "                    contours[s] = Mask(self.return_empty_mask(coordinates), resolution, coordinates)\n",
    "                    continue\n",
    "                \n",
    "                if s not in info.keys(): continue\n",
    "                \n",
    "                assigned_name = self.get_unique_structure_name(s, contours.keys())\n",
    "                data = self.get_mask(structures, s, resolution)\n",
    "                contours[assigned_name] = Mask(data, resolution, coordinates, info[s]['number'], info[s]['generation_algorithm'])\n",
    "                    \n",
    "        return contours\n",
    "                        \n",
    "    @staticmethod\n",
    "    def get_unique_structure_name(structure_name, current_structure_names):   \n",
    "        if structure_name not in current_structure_names: return structure_name \n",
    "        structure_number = 1\n",
    "        final_name = f'{mask_name}_{mask_number}'\n",
    "        while final_name in current_structure_names:\n",
    "            structure_number += 1\n",
    "            final_name = f'{structure_name}_{structure_number}'\n",
    "        return final_name\n",
    "    \n",
    "    @staticmethod\n",
    "    def return_empty_mask(coordinates):\n",
    "        dim0, dim1, dim2 = coordinates.z.shape[0], coordinates.y.shape[0], coordinates.x.shape[0]\n",
    "        return np.zeros((dim0, dim1, dim2), dtype=np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_structure(ds):\n",
    "        \"\"\"Auxiliary function for reading the content of the structure file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds : pydycom object\n",
    "            Handle for the file opened using the pydicom module \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary contraining the controu data, color, number, and name of \n",
    "            the contrours.\n",
    "        \"\"\"\n",
    "        contours = []\n",
    "         \n",
    "        for i in range(len(ds.ROIContourSequence)):\n",
    "            contour = {}\n",
    "            \n",
    "            try:\n",
    "                contour['contour_data'] = [s.ContourData for s in ds.ROIContourSequence[i].ContourSequence]\n",
    "                contour['color'] = ds.ROIContourSequence[i].ROIDisplayColor\n",
    "                contour['number'] = ds.ROIContourSequence[i].ReferencedROINumber\n",
    "                contour['name'] = str(ds.StructureSetROISequence[i].ROIName).lower()\n",
    "                contour['generation_algorithm'] = str(ds.StructureSetROISequence[i].ROIGenerationAlgorithm).lower()\n",
    "                contours.append(contour)  \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        return contours\n",
    "\n",
    "    def get_mask(self, structures, name, resolution = 'ct', method = 'interpolate'):\n",
    "          \n",
    "        print(f'Process ID: {os.getpid()} - {name}')\n",
    "        \n",
    "        method = self.mask_generation_method if method is None else method\n",
    "           \n",
    "        z = self.original_ct_coordinates.z[:] if method == 'interpolate' else self.original_dose_coordinates.z[:]\n",
    "        y_0 = self.original_ct_coordinates.image_position[1] if method == 'interpolate' else self.original_dose_coordinates.image_position[1]\n",
    "        dy = self.original_ct_coordinates.dy if method == 'interpolate' else self.original_dose_coordinates.dy\n",
    "        x_0 = self.original_ct_coordinates.image_position[0] if method == 'interpolate' else self.original_dose_coordinates.image_position[0]\n",
    "        dx = self.original_ct_coordinates.dx if method == 'interpolate' else self.original_dose_coordinates.dx\n",
    "        z_min, z_max = np.round(z.min(),self.coordinate_precision), np.round(z.max(),self.coordinate_precision)\n",
    "            \n",
    "        # allocate volume for mask using the shape of the original CT volume\n",
    "        shape = self.original_dose_shape if resolution == 'dose' and method != 'interpolate' else self.original_ct_shape\n",
    "        mask = np.zeros(shape, dtype=np.uint8) \n",
    "        \n",
    "        # round the z coordinates to avoid floating point errors\n",
    "        z = np.round(z,self.coordinate_precision)\n",
    "                  \n",
    "        ## Grab the structure data that matches the name of the desired mask\n",
    "        structure_data = [np.array(i).reshape(-1,3) for i in [s['contour_data'] for s in structures if s['name'].lower() == name.lower()][0]]\n",
    "                    \n",
    "        for nodes in structure_data: \n",
    "            \n",
    "            z_node = np.round(nodes[0, 2],self.coordinate_precision)  \n",
    "                                 \n",
    "            if np.logical_and(z_node >= z_min, z_node <= z_max): # ignore slices outside of the CT volume\n",
    "            \n",
    "                z_index = np.where(z == z_node)[0][0]\n",
    "         \n",
    "                r = (nodes[:, 1] - y_0) / dy\n",
    "                c = (nodes[:, 0] - x_0) / dx \n",
    "                \n",
    "                # make values larger than max index equal to max index\n",
    "                r[np.where(r > mask.shape[1]-1)] = mask.shape[1]-1\n",
    "                c[np.where(c > mask.shape[2]-1)] = mask.shape[2]-1\n",
    "                \n",
    "                rr, cc = polygon(r, c)\n",
    "            \n",
    "                mask[z_index, rr, cc] += 1\n",
    "          \n",
    "        mask[np.where(mask>1)] = 0 # account for holes (mask ==2) in a structure\n",
    "                      \n",
    "        if resolution == 'dose' and method == 'interpolate':\n",
    "            oc = (self.original_ct_coordinates.z, self.original_ct_coordinates.y, self.original_ct_coordinates.x)\n",
    "            ic = (self.original_dose_coordinates.z, self.original_dose_coordinates.y, self.original_dose_coordinates.x)\n",
    "            return interpolate_volume(mask, oc, ic, intMethod=self.mask_interpolation_technique, boundError=0, fillValue=0)\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def convert_hu_to_rsp(self, ct=None, in_place=True, interpolation_kind = 'linear', lut_directory=None):\n",
    "        \n",
    "        # Read LUT for HU to RSP conversion\n",
    "        if lut_directory is not None: self.rsp_lut_directory = lut_directory\n",
    "        if self.rsp_lut_directory is None: \n",
    "            self.__logger.error('No LUT directory was provided for the HU to RSP conversion.')\n",
    "            raise ValueError('No LUT directory was found or provided for the HU to RSP conversion.')\n",
    "        \n",
    "        rsp_lut_dir = os.path.join(self.rsp_lut_directory, 'mda_relative_stopping_power.csv') \n",
    "        \n",
    "        try:\n",
    "            HU_2_RSP_LUT = pandas.read_csv(rsp_lut_dir)\n",
    "        except:\n",
    "            self.__logger.error(f'Error reading the HU to RSP LUT from \"{rsp_lut_dir}\". This should be a CSV file.')\n",
    "            raise ValueError(f'Error reading the HU to RSP LUT from \"{rsp_lut_dir}\".')\n",
    "        HU = HU_2_RSP_LUT['HU'].values\n",
    "        rsp = HU_2_RSP_LUT['rsp'].values\n",
    "        \n",
    "        # create interpolating function\n",
    "        rsp2Hu = interp1d(HU, rsp, kind = interpolation_kind)\n",
    "\n",
    "        if ct is None: ct = self.ct\n",
    "        data = ct.data\n",
    "\n",
    "        # Ensure that the min and max values of CT's HU values are inside acceptable range\n",
    "        badLowInds = np.where(data < HU.min())\n",
    "        data[badLowInds] = HU.min()\n",
    "\n",
    "        badHighInds = np.where(data > HU.max())\n",
    "        data[badHighInds] = HU.max()\n",
    "\n",
    "        # Use interpolating function to convert the CT array\n",
    "        rspVol = rsp2Hu(data.flatten()).reshape(ct.shape)\n",
    "\n",
    "        if in_place: \n",
    "            ct.data = rspVol\n",
    "        else:\n",
    "            return rspVol\n",
    "\n",
    "    def __get_patient_list(self, patient_data_directory):\n",
    "        if os.path.isdir(patient_data_directory):\n",
    "            return self.identify_patient_files(patient_data_directory)\n",
    "        elif os.path.isfile(patient_data_directory) and patient_data_directory.split('.')[-1] in ['h5', 'hdf5']:\n",
    "            with h5py.File(patient_data_directory, 'r') as hf:\n",
    "                return list(hf.keys())\n",
    "        else:\n",
    "            raise Exception (f\"Unable to work with the patient data directory: {patient_data_directory}\")\n",
    "\n",
    "    def get_dicom_data_report(self, patient_list=None, save = True):\n",
    "        \n",
    "        # grab list of patient files to explore\n",
    "        if patient_list is None: \n",
    "            patient_list = self.__get_patient_list(self.patient_data_directory)\n",
    "        else:\n",
    "            if type(patient_list) != type([]): patient_list = [patient_list]\n",
    "            patient_list = sorted([str(n) for n in patient_list])\n",
    "        \n",
    "        if self.parallelize is None: self.identify_parallel_capabilitie  # check if parallel processing should be used\n",
    "        \n",
    "        def get_report(my_patients, infer_rx_dose, process_id=None, save_file = True):\n",
    "            \n",
    "            data_report = {p:{} for p in my_patients}\n",
    "             \n",
    "            # check if parallel processing is possible\n",
    "            if not self.parallelize:\n",
    "                if self.echo_progress: my_patients = tqdm(my_patients, desc=\"Generating basic report\", leave=True)\n",
    "            \n",
    "            for p in my_patients: \n",
    "                \n",
    "                self.reset()\n",
    "                                            \n",
    "                try:        \n",
    "                    self.parse_dicom_files(p, mask_names_only=True) # parse the data and grab contour names only\n",
    "                    beams = list(self.plan.beam.keys())\n",
    "                    data_report[p]['number_of_beams'] = len(beams) if beams != [] else 1\n",
    "                    data_report[p]['radiation_type'] = self.radiation_type\n",
    "                    data_report[p]['gantry_angles'] = [self.plan.beam[b].gantry_angle for b in beams]\n",
    "                    data_report[p]['couch_angles'] = [self.plan.beam[b].patient_support_angle for b in beams]\n",
    "                    data_report[p]['ct_array_dimensions'] = self.ct.data.shape\n",
    "                    data_report[p]['dose_array_dimensions'] = self.dose[beams[0]].data.shape\n",
    "                    data_report[p]['dose_array_resolution'] = {'dx':self.dose[beams[0]].coordinates.dx,\n",
    "                                                               'dy':self.dose[beams[0]].coordinates.dy,\n",
    "                                                               'dz':self.dose[beams[0]].coordinates.dz}\n",
    "                    if self.radiation_type == 'proton':\n",
    "                        data_report[p]['vsad'] = [self.plan.beam[b].vsad for b in beams]\n",
    "                    else:\n",
    "                        data_report[p]['sad'] = [self.plan.beam[b].sad for b in beams]\n",
    "                        data_report[p]['gantry_rotation_direction'] = self.dose[beams[0]].gantry_rotation_direction\n",
    "                    data_report[p]['isocenter'] = [self.plan.beam[b].isocenter for b in beams]\n",
    "                    data_report[p]['contours'] = ','.join(sorted(self.contours))\n",
    "                    data_report[p]['dose_reference_dose']= self.plan.dose_reference_dose\n",
    "                    data_report[p]['dose_reference_type']= self.plan.dose_reference_type\n",
    "                    data_report[p]['dose_reference_description']= self.plan.dose_reference_description\n",
    "                    data_report[p]['beam_type'] = list(set([self.plan.beam[b].type for b in beams]))\n",
    "                    data_report[p]['patient_position'] = self.ct.patient_position\n",
    "                                       \n",
    "                except Exception as e:\n",
    "                    self.__logger.error(f\"The data for pat-{p} could not be analyzed\")\n",
    "                    self.__logger.error(traceback.format_exc())\n",
    "                    \n",
    "            if save_file:\n",
    "                tag = f'_{process_id}' if process_id is not None else ''\n",
    "                with open(os.path.join('temp','data',f'basic_data_report{tag}.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(data_report, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        \n",
    "            if not self.parallelize: return data_report\n",
    "                \n",
    "        if self.parallelize:\n",
    "            \n",
    "            self.__logger.info(f'Parallelizing the data report generation using {self.n_threads} threads.')\n",
    "                        \n",
    "            processes = [] # initialize a list to store the processes\n",
    "                \n",
    "            # divide the patients into groups based on the number of available threads\n",
    "            chunked_patients = [x for x in np.array_split(patient_list, self.n_threads) if x.size != 0]\n",
    "            \n",
    "            # create a process for each thread\n",
    "            for n, patients in enumerate(chunked_patients):\n",
    "                p = mp.Process(target=get_report, args=(patients, infer_rx_dose, n, save,))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "            \n",
    "            for p in processes:\n",
    "                p.join()   \n",
    "            \n",
    "        else:\n",
    "            data_report = get_report(patient_list, infer_rx_dose, save_file = save)\n",
    "        \n",
    "        # merge data reports and remove temp files\n",
    "        if self.parallelize and save:\n",
    "            data_report = {}\n",
    "            for n in range(len(chunked_patients)):\n",
    "                with open(os.path.join('temp','data',f'basic_data_report_{n}.pickle'), 'rb') as handle:\n",
    "                    data_report.update(pickle.load(handle))\n",
    "                os.remove(os.path.join('temp','data',f'basic_data_report_{n}.pickle'))\n",
    "            # save merged data report\n",
    "            with open(os.path.join('temp','data','basic_data_report.pickle'), 'wb') as handle:\n",
    "                pickle.dump(data_report, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                       \n",
    "        self.__logger.info(f\"Finished generating basic report for all detected patients.\")\n",
    "\n",
    "        return data_report\n",
    "    \n",
    "    def infer_rx_dose(self, patient_list=None, min_infered_rx_dose = 20, get_dose_statistics = True):\n",
    "            \n",
    "        if patient_list is None: \n",
    "            patient_list = self.__get_patient_list(self.patient_data_directory)\n",
    "        else:\n",
    "            if type(patient_list) != type([]): patient_list = [patient_list]\n",
    "            patient_list = sorted([str(n) for n in patient_list])\n",
    "            \n",
    "        target_properties = {p : {} for p in patient_list} # TODO: change this to dose information\n",
    "        rx_dose_info ={'AID':[p for p in patient_list], 'Rx Dose':[], 'Plan Type':[], 'dose_scale':[], 'site':[]}\n",
    "        \n",
    "        if self.echo_progress: patient_list = tqdm(patient_list, desc=\"Infering prescription doses:\", leave=True)\n",
    "\n",
    "        for p in patient_list: \n",
    "                \n",
    "            self.reset()\n",
    "                                        \n",
    "            self.parse_dicom_files(p, mask_names_only=True)\n",
    "            \n",
    "            # Indentify target volumes\n",
    "            all_target_volumes = [x for x in self.contours if self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"] in x]\n",
    "            len_target_name = len(self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"])\n",
    "            target_volumes = [x for x in all_target_volumes if x[:len_target_name] == self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"]]        \n",
    "            \n",
    "            # Infer the prescription dose \n",
    "            rx = [self.__find_posible_rx_dose(x) for x in target_volumes]\n",
    "            rx = list(set([str(x) for x in rx if float(x) >=min_infered_rx_dose]))    \n",
    "            rx = [x for x in rx if x != '0']\n",
    "            if len(rx) > 1: # if multiple rx doses are found, check if they are multiples of each other\n",
    "                rx = [float(x)/100 if float(x) > 100 else float(x) for x in rx]\n",
    "                rx = [str(x) for x in set(rx)]\n",
    "                \n",
    "            target_properties[p]['infered_rx_dose'] = ','.join(rx) if rx != [] else '0'\n",
    "            target_properties[p]['all_target_like_structures'] = ','.join([x for x in self.contours if self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"] in x])\n",
    "            rx_dose_info['Rx Dose'].append(','.join(rx) if rx != [] else '0')\n",
    "            rx_dose_info['Plan Type'].append('Unknown')\n",
    "            rx_dose_info['dose_scale'].append(1.0)\n",
    "            rx_dose_info['site'].append('Unknown') \n",
    "            \n",
    "            cum_dose = self.cumulative_dose\n",
    "            \n",
    "            for t in target_volumes:\n",
    "                target_properties[p][t] = {} # TODO: use dose information variable instead of results\n",
    "                self.__find_posible_rx_dose(t)\n",
    "                # parse target mask\n",
    "                target_data = self.parse_structure_files(mask_names = t, resolution = 'dose')[t].data\n",
    "                # get the cumulative dose\n",
    "                dose_in_mask = cum_dose[np.where(target_data>0)]\n",
    "                \n",
    "                # find possible rx dose\n",
    "                rx_from_name = self.__find_posible_rx_dose(t)\n",
    "                # find dose statistics\n",
    "                target_properties[p][t]['D95'] = np.percentile(dose_in_mask, 100-max(0, min(100, 95)))\n",
    "                target_properties[p][t]['D98'] = np.percentile(dose_in_mask, 100-max(0, min(100, 98)))\n",
    "                target_properties[p][t]['D2'] = np.percentile(dose_in_mask, 100-max(0, min(100, 2)))\n",
    "                target_properties[p][t]['mean_dose'] = np.mean(dose_in_mask)\n",
    "                target_properties[p][t]['max_dose'] = np.max(dose_in_mask)\n",
    "                \n",
    "        # save target properties as json nicely formatted\n",
    "        with open(os.path.join('temp','data','target_properties.json'), 'w') as f:\n",
    "            json.dump(target_properties, f, indent=4)         \n",
    "        \n",
    "        # save rx dose information as csv\n",
    "        rx_dose_info = pandas.DataFrame(rx_dose_info)\n",
    "        rx_dose_info.to_csv(os.path.join('temp','data','rx_dose_info.csv'), index=False)\n",
    "                              \n",
    "    def __find_posible_rx_dose(self, string):\n",
    "        string = string.replace(self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"], '')\n",
    "        if 'mm' in string: return 0 # return 0 if the string contains units of mm\n",
    "        pattern = r'\\d+(?:\\.\\d+)?'  # Matches whole numbers and decimal numbers\n",
    "        matches = {float(x):x for x in re.findall(pattern, string)}\n",
    "        return matches[max(matches.keys())] if matches != {} else 0\n",
    "\n",
    "    def store_patient_data_as_hdf5(self, patient_data_directory=None, patient_list=None, mask_resolution = None, output_directory=None):\n",
    "        \n",
    "        self.mask_resolution = mask_resolution if mask_resolution is not None else 'dose'\n",
    "\n",
    "        if patient_data_directory is not None: self.patient_data_directory = patient_data_directory\n",
    "\n",
    "        if output_directory is None: \n",
    "            output_directory = os.path.join('temp','data', 'patient_data.h5')   \n",
    "\n",
    "        # Create h5file\n",
    "        if self.write_new_hdf5_file:\n",
    "            out_file = h5py.File(output_directory,'w'); out_file.close()\n",
    "\n",
    "        if patient_list is None: patient_list = self.identify_patient_files()\n",
    "\n",
    "        if self.echo_progress: patient_list = tqdm(patient_list, desc=\"Saving HDF5 patient file\", leave=True)\n",
    "        \n",
    "        for p in patient_list:\n",
    "\n",
    "            self.patient_id, files = p, self.run_initial_check(p)\n",
    "            \n",
    "            self.ct = self.parse_ct_study_files(files['ct'])\n",
    "\n",
    "            self.append_data_to_hdf5_file(output_directory, data_type='ct')\n",
    "            \n",
    "            # Parse the dose volime and (optinally) the plan \n",
    "            if 'plan' in files.keys() and files['plan'] != []:\n",
    "                self.dose, self.plan = self.parse_rt_dose_files(files['dose'], files['plan'])\n",
    "            else:\n",
    "                self.dose = self.parse_rt_dose_files(files['dose'])\n",
    "                self.plan = Plan()\n",
    "\n",
    "            self.append_data_to_hdf5_file(output_directory, data_type='dose')\n",
    "\n",
    "            # Parse the contours\n",
    "            for f in files['structures']:\n",
    "            \n",
    "                self.contours = self.parse_structure_files([f], resolution=self.mask_resolution)\n",
    "\n",
    "                self.append_data_to_hdf5_file(output_directory, 'contours')\n",
    "\n",
    "            self.parse_dicom_files(p)\n",
    "\n",
    "        self.__logger.info(f\"The patient data was stored as an HDF5 file in {output_directory}.\")\n",
    "    \n",
    "    def append_data_to_hdf5_file(self, output_directory, data_type):\n",
    "        \"\"\"Append the data for each patient to an HDF5 file for faster I/O\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `output_directory` : str\n",
    "            directory for the folder where the patient data file will be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        if data_type == 'ct':\n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "\n",
    "                # CT data\n",
    "                dset = f.create_dataset(name='/'.join([self.patient_id, 'ct']), data = self.ct.data, compression = self.compression)\n",
    "                dset.attrs['units'] = self.ct.units\n",
    "                dset.attrs['rescale_slope'] = self.ct.rescale_slope\n",
    "                dset.attrs['rescale_intercept'] = self.ct.rescale_intercept\n",
    "                dset.attrs['units'] = self.ct.units\n",
    "                dset.attrs['resolution'] = self.ct.resolution\n",
    "                dset.attrs['x'] = self.ct.coordinates.x\n",
    "                dset.attrs['y'] = self.ct.coordinates.y\n",
    "                dset.attrs['z'] = self.ct.coordinates.z\n",
    "                dset.attrs['dx'] = self.ct.coordinates.dx\n",
    "                dset.attrs['dy'] = self.ct.coordinates.dy\n",
    "                dset.attrs['dz'] = self.ct.coordinates.dz\n",
    "                dset.attrs['image_position'] = self.ct.coordinates.image_position\n",
    "\n",
    "        elif data_type == 'dose':\n",
    "                \n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "                \n",
    "                # Dose data\n",
    "                if self.radiation_type.lower() == 'pronton':\n",
    "                    for b in self.dose.keys():\n",
    "                        dset = f.create_dataset(name='/'.join([self.patient_id, 'dose', str(b)]), data = self.dose[b].data, compression = self.compression )\n",
    "                        dset.attrs['dose_grid_scaling'] = self.dose[b].dose_grid_scaling\n",
    "                        dset.attrs['dose_units'] = self.dose[b].dose_units\n",
    "                        dset.attrs['beam_type'] = self.dose[b].beam_type\n",
    "                        dset.attrs['vsad'] = self.dose[b].vsad\n",
    "                        dset.attrs['number_of_control_points'] = self.dose[b].number_of_control_points\n",
    "                        dset.attrs['gantry_angle'] = self.dose[b].gantry_angle\n",
    "                        dset.attrs['patient_support_angle'] = self.dose[b].patient_support_angle\n",
    "                        dset.attrs['table_top_pitch_angle'] = self.dose[b].table_top_pitch_angle\n",
    "                        dset.attrs['table_top_roll_angle'] = self.dose[b].table_top_roll_angle\n",
    "                        dset.attrs['isocenter'] = self.dose[b].isocenter\n",
    "                        dset.attrs['resolution'] = self.dose[b].resolution\n",
    "                        dset.attrs['x'] = self.dose[b].coordinates.x\n",
    "                        dset.attrs['y'] = self.dose[b].coordinates.y\n",
    "                        dset.attrs['z'] = self.dose[b].coordinates.z\n",
    "                        dset.attrs['dx'] = self.dose[b].coordinates.dx\n",
    "                        dset.attrs['dy'] = self.dose[b].coordinates.dy\n",
    "                        dset.attrs['dz'] = self.dose[b].coordinates.dz\n",
    "                        dset.attrs['image_position'] = self.dose[b].coordinates.image_position\n",
    "                        \n",
    "                #TODO: add values for photon dose\n",
    "                \n",
    "                # Plan data\n",
    "                plan = f.create_group(name=f'{self.patient_id}/plan')\n",
    "                plan.attrs['number_of_beams'] = self.plan.number_of_beams\n",
    "                plan.attrs['plan_label'] = self.plan.plan_label\n",
    "                plan.attrs['patient_sex'] = self.plan.patient_sex\n",
    "                plan.attrs['plan_name'] = self.plan.plan_name\n",
    "\n",
    "        elif data_type == 'contours':\n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "                for c in self.contours.keys():\n",
    "                    name = str(c).replace(\"/\",\"-\")\n",
    "                    dset = f.create_dataset(name='/'.join([self.patient_id, 'contours', self.contours[c].structure_set, str(name)]), \n",
    "                                            data = self.contours[c].data, compression = self.compression )\n",
    "                    dset.attrs['resolution'] = self.contours[c].resolution\n",
    "\n",
    "    def read_data_from_hdf5(self, patient_id = None, contours_type='clinical'):\n",
    "        \"\"\"Read patient data from HDF5 files generated by this class using  the `store_patient_data_as_hdf5` method. \n",
    "        This allows for fast I/O but increases the hard-drive memory consumption by creating (in many cases) a large \n",
    "        HDF5 file with all of the patient data. Please ensure that enough memory is available when using this \n",
    "        method of data wrangling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `patient_id` : str, optional\n",
    "            ID of the patient data to read, by default None\n",
    "        \"\"\"\n",
    "\n",
    "        if patient_id is not None: self.patient_id = patient_id\n",
    "\n",
    "        with h5py.File(self.user_inputs[\"DIRECTORIES\"][\"raw_patient_data\"],'r') as f:\n",
    "\n",
    "            # Plan\n",
    "            plan = f[f'{self.patient_id}/plan']\n",
    "            number_of_beams = plan.attrs['number_of_beams']\n",
    "            patient_sex = plan.attrs['patient_sex']            \n",
    "            plan_label = plan.attrs['plan_label'] \n",
    "            plan_name = plan.attrs['plan_name']\n",
    "            modality = plan.attrs['modality']\n",
    "            self.plan = Plan(number_of_beams, patient_sex, plan_label, plan_name, modality)\n",
    "\n",
    "            # CT\n",
    "            params = [f[f'{self.patient_id}/ct'][...].shape]\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['resolution'])\n",
    "            params.append(f[f'{self.patient_id}/ct'][...].max())\n",
    "            params.append(f[f'{self.patient_id}/ct'][...].min())\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['units'])\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['rescale_slope'])\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['rescale_intercept'])\n",
    "            params.append(f[f'{self.patient_id}/ct'][...])\n",
    "            x, y, z = f[f'{self.patient_id}/ct'].attrs['x'], f[f'{self.patient_id}/ct'].attrs['y'], f[f'{self.patient_id}/ct'].attrs['z']\n",
    "            dx, dy, dz = f[f'{self.patient_id}/ct'].attrs['dx'], f[f'{self.patient_id}/ct'].attrs['dy'], f[f'{self.patient_id}/ct'].attrs['dz']\n",
    "            image_position = f[f'{self.patient_id}/ct'].attrs['image_position']\n",
    "            self.original_ct_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "            params.append(self.original_ct_coordinates)\n",
    "            self.ct = CT(*params)\n",
    "\n",
    "            # Dose\n",
    "            self.dose = {int(bn):None for bn in f[f'{self.patient_id}/dose'].keys()}\n",
    "            for k in self.dose.keys():\n",
    "                params = [f[f'{self.patient_id}/dose/{k}'][...].shape]\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...].max())\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...].min())\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['resolution'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['dose_grid_scaling'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['dose_units'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...])\n",
    "                x, y, z = f[f'{self.patient_id}/dose/{k}'].attrs['x'], f[f'{self.patient_id}/dose/{k}'].attrs['y'], f[f'{self.patient_id}/dose/{k}'].attrs['z']\n",
    "                dx, dy, dz = f[f'{self.patient_id}/dose/{k}'].attrs['dx'], f[f'{self.patient_id}/dose/{k}'].attrs['dy'], f[f'{self.patient_id}/dose/{k}'].attrs['dz']\n",
    "                image_position = f[f'{self.patient_id}/dose/{k}'].attrs['image_position']\n",
    "                self.original_dose_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "                params.append(self.original_dose_coordinates)\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['modality'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['beam_type'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['vsad'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['number_of_control_points'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['gantry_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['patient_support_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['table_top_pitch_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['table_top_roll_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['isocenter'])\n",
    "                self.dose[k] = Dose(*params)\n",
    "                \n",
    "            #TODO: differentiate between photon and proton dose\n",
    "\n",
    "            # Contours\n",
    "            self.contours = {c:None for c in  f[f'{self.patient_id}/contours/{contours_type}'].keys()}\n",
    "            for k in self.contours.keys():\n",
    "                data = f[f'{self.patient_id}/contours/{contours_type}/{k}'][...]\n",
    "                resolution = f[f'{self.patient_id}/contours/{contours_type}/{k}'].attrs['resolution']\n",
    "                if resolution == 'ct':\n",
    "                    coordinates = self.original_ct_coordinates\n",
    "                else:\n",
    "                    coordinates = self.original_dose_coordinates\n",
    "                self.contours[k] = Mask(data, resolution, coordinates)\n",
    "            # Grab CTVs in case the desired contours are automatic\n",
    "            if contours_type == 'auto':\n",
    "                for k in [c for c in f[f'{self.patient_id}/contours/clinical'].keys() if 'ctv' in c and 'fsctv' not in c and 'pctv' not in c]:\n",
    "                    data = f[f'{self.patient_id}/contours/clinical/{k}'][...]\n",
    "                    resolution = f[f'{self.patient_id}/contours/clinical/{k}'].attrs['resolution']\n",
    "                    coordinates = self.original_ct_coordinates if resolution == 'ct' else self.original_dose_coordinates\n",
    "                    self.contours[k] = Mask(data, resolution, coordinates)\n",
    "\n",
    "    @property\n",
    "    def cumulative_dose(self):\n",
    "        \"\"\"Compute the cumulative dose volume for all of the beams in the plan.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            3D array containing the cumulative dose volume.\n",
    "        \"\"\"\n",
    "        return np.sum([self.dose[b].data for b in self.dose.keys()], axis = 0)\n",
    "\n",
    "    @property\n",
    "    def identify_parallel_capabilities(self):\n",
    "        \n",
    "        self.n_threads = self.user_inputs['PARALLELIZATION']['number_of_processors'] \n",
    "        if self.n_threads is None: self.n_threads = mp.cpu_count()//2 # assumes that half of the CPUs are available for parallel processing\n",
    "        if self.n_threads >  mp.cpu_count():\n",
    "            self.__logger.warning(f\"The number of threads ({self.n_threads}) exceeds the number of available CPUs ({mp.cpu_count()}).\")\n",
    "            self.n_threads = mp.cpu_count()//2\n",
    "            self.__logger.warning(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        self.parallelize = True if self.user_inputs['PARALLELIZATION']['parallelize_data_preprocessing'] and self.n_threads > 1 else False   \n",
    "        \n",
    "        if not self.parallelize:\n",
    "            self.n_threads = 1\n",
    "            self.__logger.info(\"Preprocessing will be performed sequentially. Consider using parallel processing to speed up the process.\")\n",
    "            return\n",
    "\n",
    "        # log the available resources\n",
    "        self.__logger.info(f\"Number of CPUs (Virtual): {mp.cpu_count()}\")\n",
    "        \n",
    "        # get the number of threads ready to perform work concurrently\n",
    "        if self.user_inputs['PARALLELIZATION']['number_of_processors'] is None and self.parallelize:\n",
    "            self.n_threads = mp.cpu_count()//2 # assumes that half of the CPUs are available for parallel processing\n",
    "            self.__logger.info(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        else:\n",
    "            self.n_threads = int(self.user_inputs['PARALLELIZATION']['number_of_processors'])\n",
    "            if self.parallelize and self.n_threads > mp.cpu_count():\n",
    "                self.__logger.warning(f\"The number of threads ({self.n_threads}) exceeds the number of available CPUs ({mp.cpu_count()}).\")\n",
    "                self.__logger.warning(f\"Parallel processing will be used with {mp.cpu_count()} threads\")\n",
    "                self.n_threads = mp.cpu_count()\n",
    "            elif self.n_threads == 1:\n",
    "                self.parallelize = False\n",
    "                self.__logger.info(\"Parallel processing will not be used since only 1 thread was requested\")\n",
    "        \n",
    "        if self.n_threads > 1 and self.parallelize:\n",
    "            self.__logger.info(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        else:       \n",
    "            self.__logger.info(\"Preprocessing will be performed sequentially. Consider using parallel processing to speed up the process.\")\n",
    "    \n",
    " \n",
    "user_inputs_dir = \"configuration_files/user_config.json\"\n",
    "dt = DicomToolbox(user_inputs_dir)\n",
    "patient_ID = 1\n",
    "\n",
    "dt.parse_dicom_files(patient_ID, mask_resolution='dose')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa31c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.contours.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./rt_utils\")\n",
    "\n",
    "import RTStructBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812d896",
   "metadata": {},
   "source": [
    "**Creating a basic report**\n",
    "\n",
    "Several of the routines used for preprocessing rely on a *basic report* that can be created with the `DicomToolbox` class. The basic report is created by executing the `get_dicom_data_report()` method. This is normally done automatically by the classes and should be renewed for every new dataset. Currently, the report needs to be deleted manually when the data changes. In the next version, we will add a check to see if the data has changed and renew the report automatically.\n",
    "\n",
    "\n",
    "The report is helpful to quickly inspect a set of patients. You can give a list of patients as an input with the `patient_list` variable if you want to run this method on a specific set. If a list of patients in not given, the method can use the `identify_patient_files()` to detect all patients with a complete set of DICOM-RT files. This method returns a dictionary with the result of the basic report. The result is also saved as a pickle file in the temporary folder. \n",
    "\n",
    "**Important**: If you do not have the prescription values for the patients, you can set the `infer_rx_dose` flag to `True` when calling the `create_basic_report()` method. This will infer the prescription dose from the structure names. This is not a perfect method, but it is helpful when you do not have the prescription dose for the patients. You can further refine the file produced by manually editing the CSV file produced when infering the prescription dose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_report = dt.get_dicom_data_report()\n",
    "\n",
    "# basic_report = dt.get_dicom_data_report(patient_list=['67'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0228bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.infer_rx_dose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in basic_report.keys():\n",
    "    print(k, basic_report[k]['beam_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f2279b",
   "metadata": {},
   "source": [
    "For your convinience, you can quickly check the name of potential target volumes for the patients. The result will be saved in a file called `target_report.json` in the `temp/data` directory. Note that the candidate target volumes are found based on the value you specify for `TYPE_OF_TARGET_VOLUME` in the configuration JSON file. Once the file is generated, you can open it and compare it to the file infering the prescription dose. This should help you clean the prescription dose file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = list(basic_report.keys())\n",
    "patient_ids.sort(key=natural_keys)\n",
    "target_report = {p:basic_report[p]['candidate_target_volumes'].split(\",\") for p in patient_ids}\n",
    "\n",
    "with open(os.path.join('temp', 'data', 'target_report.json'), 'w') as fp:\n",
    "    json.dump(target_report, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eccf296-132e-4c02-a915-decaaf4fa73e",
   "metadata": {},
   "source": [
    "### Storing the patient data as a single HDF5 file\n",
    "\n",
    "**Not available for Photon data yet**\n",
    "\n",
    "In some circumstances, it may be convinient to save the data as a single HDF5 file to expedite the development of new routine. This is because the I/O of patient data from an HDF5 file is many times faster than reading data from raw DICOM files in python. The `DicomToolbox` class was designed to know how to obtain all of the relevant patient information from HDF5 files it creates. This files contain all of the details needed by other classes that later perform the steps involved in preprocessing and creating the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61680bb-4831-4d12-865e-513939f94877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.store_patient_data_as_hdf5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d5c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# with h5py.File('temp/data/patient_data.h5', 'r') as hf:\n",
    "#     print(hf['16/contours/clinical'].keys())\n",
    "#     ct = hf['16/ct'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "dose = np.array([25.03309, 25.03309, 50.06618, 50.06618, 75.09927, 75.09927, 100.1324, 100.1324, 149.8357, 149.8357, 199.9019, 249.9681, 249.9681, 300.0343, 350.1005, 350.1005])\n",
    "reading = np.array([88918, 89803, 179272, 180574, 271044, 272178, 364934, 361838, 551177, 555773, 744155, 949256, 943599, 1133267, 1354215, 1366654])\n",
    "\n",
    "# Fit a linear model to the data\n",
    "coeffs = np.polyfit(dose, reading, 1)\n",
    "\n",
    "# Generate y-values based on the fit\n",
    "fit = np.polyval(coeffs, dose)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(dose, reading, label='Data')\n",
    "\n",
    "# Plot the fit\n",
    "plt.plot(dose, fit, 'r-', label='Fit: a=%5.3f, b=%5.3f' % (coeffs[0], coeffs[1]))\n",
    "\n",
    "plt.xlabel('Dose (cGy)')\n",
    "plt.ylabel('Reading')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab521790-96a3-49d4-a842-48335127aa80",
   "metadata": {},
   "source": [
    "---\n",
    "## The DataExplorer package\n",
    "\n",
    "This class was developed to help explore a set of patient folders. The `DataExplorer` class inherits the content of the `DicomToolbox` class and can, therefore, use its functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_explorer import DataExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815f01c-4be3-4bcf-896b-e0bf4de24004",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = DataExplorer(user_inputs_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "355f03a5-253b-432f-8616-bab5a30ba1ee",
   "metadata": {},
   "source": [
    "### Creating a human friendly report for the DICOM data\n",
    "The `create_data_report()` method in the `DataExplorer` class generates a \"csv\" file containing a report of the patient data. This report is also stored as a class variable for other methods to use. For example, the method used to filter the patients relies on this report to detect patients with desirable characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b4210-2a35-4919-b18b-590b61fefc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.create_data_report(save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaf472a5-4568-455b-b0e1-f18bd8a0ed97",
   "metadata": {},
   "source": [
    "### Filtering Patients\n",
    "The `DataExplorer` class can filter patients based on the properties of their data, like the number of beams used in the plan and the geometries of the beams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84b686-822e-46dd-a41e-c42b76dae485",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = de.apply_patient_filter()\n",
    "\n",
    "print(f'No. of selected patients: {len(selected)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e4da450-36bd-4ef6-8ba0-8e8763b57bbe",
   "metadata": {},
   "source": [
    "### Generate a report of the available contours\n",
    "\n",
    "With this class, a comprehensive report of the available contours can be generated. This report is particularly useful when adding new patients to the data pool since it allows you to compare the name of the contours that were selected with those available for each patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b9c17-2c2f-4c65-a89e-686847e8f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.generate_contour_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb60da9",
   "metadata": {},
   "source": [
    "Run the cell below for an example on how to find targets for a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.find_targets(pat_id=159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbeca5",
   "metadata": {},
   "source": [
    "Run the cell below for an example on how to find OARs for a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.find_oars(pat_id=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9906a3",
   "metadata": {},
   "source": [
    "The output of this method consists of four JSON files describing the available and selected contours for the patients. The files are saved in the `temp/data` folder. You should carefully assess the quality of the picks before performing preprocessing. Since it is unlikely that we have captured all variations for the names of the contours, you may need to make changes to the code, the files used to find structures, or add structures manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178315d",
   "metadata": {},
   "source": [
    "---\n",
    "### Analyzing the data further\n",
    "\n",
    "The routines below can help you get a sense for the properties of your data. They are not part of the preprocessing pipeline, but they can help you understand the data better.\n",
    "\n",
    "#### Gender information\n",
    "\n",
    "This will not be informative if the gender was annonimized. You may need to run this on the original DICOM files. If the original data is in a directory different than the one in your `user_config` file, you can specify the path to the data in the `DATA_DIRECTORY` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977675f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pydicom import dcmread\n",
    "import pickle\n",
    "\n",
    "DATA_DIRECTORY = dt.patient_data_directory  # directory for data containing gender information\n",
    "\n",
    "with open(os.path.join('temp', 'data', 'basic_data_report.pickle'), 'rb') as f:\n",
    "    basic_report = pickle.load(f) # baic data report used by some of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf58112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_sex(file_path):\n",
    "    try:\n",
    "        ds = dcmread(file_path)\n",
    "        return ds.PatientSex if 'PatientSex' in ds else None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "genders = []\n",
    "for folder in os.listdir(DATA_DIRECTORY):\n",
    "    patient_folder = os.path.join(DATA_DIRECTORY, folder)\n",
    "    gender_found = False\n",
    "    for root, dirs, files in os.walk(patient_folder):\n",
    "        if gender_found: break\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            gender = get_patient_sex(file_path)\n",
    "            if gender is not None:\n",
    "                genders.append(gender)\n",
    "                gender_found = True\n",
    "                break\n",
    "        \n",
    "# Count unique gender types\n",
    "gender_types, gender_type_counts = np.unique(genders, return_counts=True)\n",
    "# Create a pie-chart with patient gender\n",
    "plt.pie(gender_type_counts, labels=gender_types, autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table showing patient positions \n",
    "positions = [basic_report[patient]['patient_position'] for patient in basic_report]\n",
    "position_types, position_type_counts = np.unique(positions, return_counts=True)\n",
    "position_dict = dict(zip(position_types, position_type_counts))\n",
    "position_df = pd.DataFrame.from_dict(position_dict, orient='index', columns=['Count'])\n",
    "position_df.index.name = 'Patient Position'\n",
    "position_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table showing the beam types \n",
    "beam_types = [basic_report[patient]['beam_type'] for patient in basic_report]\n",
    "beam_type_types, beam_type_type_counts = np.unique(beam_types, return_counts=True)\n",
    "beam_type_dict = dict(zip(beam_type_types, beam_type_type_counts))\n",
    "beam_type_df = pd.DataFrame.from_dict(beam_type_dict, orient='index', columns=['Count'])\n",
    "beam_type_df.index.name = 'Beam Type'\n",
    "beam_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f388f3c",
   "metadata": {},
   "source": [
    "### Get the size of the dose grid for the selected and entire patient population\n",
    "\n",
    "The code below will inspect the dimensions (number of voxels) and resolution (voxel size) of the dose grid for the selected patients and the entire patient population. This is useful to check if the dose grid is consistent across patients. If the dose grid is not consistent, you may need to resample the dose grid to a common resolution. This is currently an option during preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dose_grid_dimensions = {'z':[], 'y':[], 'x':[]}\n",
    "for p in selected:\n",
    "    dose_grid_dimensions['z'].append(basic_report[p]['dose_array_dimensions'][0])\n",
    "    dose_grid_dimensions['y'].append(basic_report[p]['dose_array_dimensions'][1])\n",
    "    dose_grid_dimensions['x'].append(basic_report[p]['dose_array_dimensions'][2])\n",
    "    \n",
    "# generate seaborn plot for the dose grid dimensions side by side\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(dose_grid_dimensions['z'], alpha=0.5, label='z', color='red')\n",
    "plt.xlabel('z dimension')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(dose_grid_dimensions['y'], alpha=0.5, label='y', color='green')\n",
    "plt.xlabel('y dimension')\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(dose_grid_dimensions['x'], alpha=0.5, label='x', color='blue')\n",
    "plt.xlim(np.min(dose_grid_dimensions['x']))\n",
    "plt.xlabel('x dimension');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a693a",
   "metadata": {},
   "source": [
    "#### Voxel size analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the maximum size for each dimension\n",
    "print(f'Maximum size for z-axis: {np.max(dose_grid_dimensions[\"z\"])}')\n",
    "print(f'Maximum size for y-axis: {np.max(dose_grid_dimensions[\"y\"])}')\n",
    "print(f'Maximum size for x-axis: {np.max(dose_grid_dimensions[\"x\"])}')\n",
    "\n",
    "dose_grid_dimensions = {'dz':[], 'dy':[], 'dx':[]}\n",
    "for p in basic_report.keys():\n",
    "    dose_grid_dimensions['dz'].append(basic_report[p]['dose_array_resolution']['dz'])\n",
    "    dose_grid_dimensions['dy'].append(basic_report[p]['dose_array_resolution']['dy'])\n",
    "    dose_grid_dimensions['dx'].append(basic_report[p]['dose_array_resolution']['dx'])\n",
    "    \n",
    "# print a table of the dose grid resolution\n",
    "df = pd.DataFrame(dose_grid_dimensions)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4fd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dose grid resolution\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(dose_grid_dimensions['dz'], alpha=0.5, label='z', color='red')\n",
    "plt.xlabel('z resolution')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(dose_grid_dimensions['dy'], alpha=0.5, label='y', color='green')\n",
    "plt.xlabel('y resolution')\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(dose_grid_dimensions['dx'], alpha=0.5, label='x', color='blue')\n",
    "plt.xlabel('x resolution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819fc6b",
   "metadata": {},
   "source": [
    "### Perform an evaluation of the dose properties\n",
    "\n",
    "This method checks the properties of the dose inside the target volumes and OARs to help identify potential issues like incorrectly assigned prescription dose values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d191f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.get_dose_property_reports(target_metrics={'D':[95,98,99, 'max', 'mean'],'V':[95, 100, 105]}, oar_metrics = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "de.get_dose_metric_report_as_csv(target_analysis_dir = 'temp/data/target_dose_analysis.json', metric='V_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8866f13",
   "metadata": {},
   "source": [
    "### Get information about the contours available for the selected patients\n",
    "\n",
    "You can visualize the frequency of some of the OARs for your patients by running the cell below. This will generate a bar plot with the frequency of the OARs in the selected patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read report of the oars found\n",
    "with open(os.path.join('utilities', 'liver', 'selected_oars_report.json'), 'r') as f:\n",
    "    selected_oars_report = json.load(f)\n",
    "\n",
    "contours_frequency_report = {c:0 for c in selected_oars_report[list(selected_oars_report.keys())[0]].keys()}\n",
    "\n",
    "for p in selected_oars_report.keys():\n",
    "    for c in selected_oars_report[p].keys():\n",
    "        if selected_oars_report[p][c] != 0:\n",
    "            contours_frequency_report[c]+=1\n",
    "            \n",
    "# create histogram of the frequency of each contour\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(contours_frequency_report.keys(), contours_frequency_report.values(), color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of each contour');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74d4171d-f79c-408c-8c9d-f44d3ee9d16f",
   "metadata": {},
   "source": [
    "---\n",
    "## The PreProcessing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a503e6-ca7d-4279-92d7-05957f15e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs_dir = \"configuration_files/user_config.json\"\n",
    "dp = PreProcessing(user_inputs_dir)\n",
    "# dp.prepare_training_data(selected_patients = [236]) # you can specify the patients you want to prepare\n",
    "dp.prepare_training_data(use_contour_record = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in dp.plan.beam.keys():\n",
    "    print(n, dp.plan.beam[n])\n",
    "    # print(dp.plan.beam[n].sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'black', 'yellow', 'lime', \n",
    "          'navy', 'teal', 'maroon', 'aqua', 'fuchsia', 'silver', 'gold', 'coral', 'beige', 'mint', 'lavender']\n",
    "\n",
    "with h5py.File('temp/data/preprocessed.h5', 'r') as f:\n",
    "    print(len(f.keys()))\n",
    "    dose = f['1/dose'][:]\n",
    "    ct = f['1/ct'][:]\n",
    "  \n",
    "    plt.imshow(ct[100,:,:], cmap = 'gray')\n",
    "    plt.imshow(dose[100,:,:], cmap = 'jet', alpha = 0.5) \n",
    "    for n,c in enumerate(f['1/contours'].keys()):\n",
    "        print(n,c, colors[n])\n",
    "        plt.contour(f['1/contours'][c][100,:,:], colors=[colors[n]])\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "with h5py.File('temp/data/preprocessed.h5', 'r') as hf:\n",
    "    print(hf['123/contours'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f11088",
   "metadata": {},
   "source": [
    "___ \n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging, logging.config, random, argparse\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # could be given with argparse\n",
    "import tensorflow as tf\n",
    "from input_pipeline_tools import InputPipelineTools\n",
    "from training_pipeline_tools import TrainingPipelineTools\n",
    "import numpy as np\n",
    "from tfrecords_parser import parse_training_tfrecords, parse_validation_tfrecords, random_occlusion, augment_dataset\n",
    "\n",
    "# Initialize the input pipeline tools \n",
    "ipt = InputPipelineTools(user_inputs_dir, output_dir=os.path.join('temp', 'data')) \n",
    "tpt = TrainingPipelineTools(user_inputs_dir) \n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(ipt.seed)\n",
    "np.random.seed(ipt.seed)\n",
    "random.seed(ipt.seed)\n",
    "\n",
    "# Preparing data files for model\n",
    "## Creating new TFRecords\n",
    "if ipt.load_data_split and ipt.data_split_info_dir != \"none\": \n",
    "    ipt.get_data_split_info(ipt.data_split_info_dir)\n",
    "else: \n",
    "    ipt.create_data_splits()\n",
    "\n",
    "ipt.prepare_tfrecords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f88499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from TFRecords\n",
    "train_ds = tf.data.Dataset.list_files(os.path.join(f\"{ipt.train_dir}\",\"*.tfrec\"), shuffle=True)\n",
    "val_ds = tf.data.Dataset.list_files(os.path.join(f\"{ipt.val_dir}\",\"*.tfrec\"), shuffle=False)\n",
    "\n",
    "# Interleave TFRecrodDataset function to read files\n",
    "train_ds = train_ds.interleave(tf.data.TFRecordDataset, num_parallel_calls = tf.data.AUTOTUNE, deterministic=False)\n",
    "val_ds = val_ds.interleave(tf.data.TFRecordDataset, num_parallel_calls = tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "# Data parsing and augmentation\n",
    "## Training set\n",
    "parsing_function = lambda X: parse_training_tfrecords(X,\n",
    "                                                        features_in_input = tf.constant([x for x in ipt.inputs], dtype=tf.string),\n",
    "                                                        structures_in_target = tf.constant([x for x in ipt.targets if 'contours/' in x], dtype=tf.string),\n",
    "                                                        include_voi_weights = tf.constant(ipt.use_weight_matrix))\n",
    "\n",
    "augmentation_function = lambda inputs, targets, tlc, brc: augment_dataset(inputs, targets, tlc, brc, \n",
    "                                                                            input_size = len(ipt.inputs),\n",
    "                                                                            target_size = len(ipt.targets),\n",
    "                                                                            aug_parameters = ipt.augmentation_details)\n",
    "\n",
    "rw_prob = ipt.augmentation_parameters['random_occlusion']['probability']\n",
    "rw_size = ipt.augmentation_parameters['random_occlusion']['window_size']\n",
    "random_occlusion_fn = lambda inputs, targets: random_occlusion(inputs, targets, size = tf.constant(rw_size, dtype=tf.int64), probability = tf.constant(rw_prob, dtype=tf.float32))\n",
    "\n",
    "# Apply random occlusion patch inside of training data if specified\n",
    "if 'random_occlusion' in  ipt.augmentation_types:\n",
    "    rw_prob = ipt.augmentation_parameters['random_occlusion']['probability']\n",
    "    rw_size = ipt.augmentation_parameters['random_occlusion']['window_size']\n",
    "    random_occlusion_fn = lambda inputs, targets: random_occlusion(inputs, targets, \n",
    "                                                                    size = tf.constant(rw_size, dtype=tf.int64), \n",
    "                                                                    probability = tf.constant(rw_prob, dtype=tf.float32))\n",
    "\n",
    "# Read the data from the TFRecordDataset for every patient in the training set\n",
    "train_ds = train_ds.map(parsing_function, num_parallel_calls = tf.data.AUTOTUNE, deterministic=False)\n",
    "if ipt.cache_training_dataset: train_ds = train_ds.cache() # cache the data for faster training\n",
    "\n",
    "# Apply augmentation\n",
    "train_ds = train_ds.map(augmentation_function, num_parallel_calls = tf.data.AUTOTUNE, deterministic=False)\n",
    "if 'random_occlusion' in ipt.augmentation_types: train_ds = train_ds.map(random_occlusion_fn, num_parallel_calls = tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "# Shuffle, repeat, batch and prefetch the data\n",
    "train_ds = train_ds.shuffle(buffer_size=ipt.shuffle_buffer_size)\n",
    "train_ds = train_ds.repeat(ipt.dataset_repeats[\"training-set\"]) \n",
    "train_ds = train_ds.batch(batch_size=ipt.batch_size, drop_remainder = True)\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "## Validation set\n",
    "parsing_function = lambda X: parse_validation_tfrecords(X, \n",
    "                                                        input_size = len(ipt.inputs),\n",
    "                                                        target_size = len(ipt.targets),\n",
    "                                                        features_in_input = tf.constant([x for x in ipt.inputs], dtype=tf.string),\n",
    "                                                        structures_in_target = tf.constant([x for x in ipt.targets if 'contours/' in x], dtype=tf.string),\n",
    "                                                        include_voi_weights = tf.constant(ipt.use_weight_matrix))\n",
    "\n",
    "val_ds = val_ds.map(parsing_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "if ipt.cache_validation_dataset: val_ds = val_ds.cache()\n",
    "val_ds = val_ds.batch(batch_size=ipt.batch_size, drop_remainder = True)\n",
    "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# TRAINING PIPELINE\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    \n",
    "    model = tpt.get_model()\n",
    "    model.compile(optimizer=tpt.get_optimizer(),\n",
    "                    loss=tpt.get_loss_function())\n",
    "\n",
    "# Train the model (with or without profiler)\n",
    "if tpt.activate_profiler : tf.profiler.experimental.start('profiler_lok_dir')\n",
    "\n",
    "history = model.fit(train_ds,   \n",
    "                    epochs=ipt.epochs,\n",
    "                    validation_data=val_ds,\n",
    "                    callbacks = tpt.callbacks)\n",
    "\n",
    "if tpt.activate_profiler: tf.profiler.experimental.stop()\n",
    "\n",
    "# generate a plot of the training history\n",
    "tpt.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c457d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a plot of the training history\n",
    "tpt.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff21a3",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557173df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import evaluate_model\n",
    "\n",
    "# evaluate_model(user_inputs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_toolbox import ModelEvaluationTools\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "echo_progress = True\n",
    "user_inputs_dir = \"configuration_files/user_config.json\"\n",
    "\n",
    "met = ModelEvaluationTools(user_inputs_dir)\n",
    "\n",
    "# create model inference folder\n",
    "met.prepare_inference_folder\n",
    "\n",
    "\n",
    "# Determine the fold(s) to evaluate\n",
    "folds = met.eval_folds if met.eval_folds is not None else 0\n",
    "\n",
    "# Weight directory\n",
    "weights_dir = met.weights_dir if met.weights_dir is not None else \"\"\n",
    "\n",
    "# Sanity checks\n",
    "if type(folds) == type([]) and len(folds)>1:\n",
    "    if met.data_set_to_evaluate.lower() != 'test':\n",
    "        msg = 'Multi-fold evaluation only makes sense on the test set. Give only one fold or change the data set to test.'\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "# prepare the evaluation data\n",
    "eval_data_set = met.read_data_ids(data_type = met.data_set_to_evaluate)\n",
    "\n",
    "pat_eval = {p:{} for p in eval_data_set}  \n",
    "gt_norm_fctr, pr_norm_fctr = 1.0, 1.0\n",
    "patient_ids = tqdm(eval_data_set) if echo_progress else eval_data_set\n",
    "\n",
    "# prepare the model if only one fold is used and a prediction file is to be written\n",
    "if len(folds) == 1 and met.prediction_file_mode in ['w', 'a']: \n",
    "    model = met.prepare_model(folds[0], weights_dir, compiled=True)\n",
    "else:\n",
    "    model = None\n",
    "    \n",
    "# Evaluate the prediction for each patient in the evaluation set \n",
    "for p in patient_ids:\n",
    "    \n",
    "    # get the dose distribution for the patient\n",
    "    gt_dose, pr_dose = met.get_dose_volumes(p, folds, weights_dir, model)\n",
    "                                    \n",
    "    # Normalize the dose if desired\n",
    "    gt_dose, pr_dose, gt_norm_fctr, pr_norm_fctr = met.normalize_dose(p, gt_dose, pr_dose, \n",
    "                                                                        method=met.normalization_method, \n",
    "                                                                        normalize=met.reference_normalization_volume)\n",
    "        \n",
    "    # Save the prediction\n",
    "    if met.prediction_file_mode == 'w':\n",
    "        met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)\n",
    "    elif met.prediction_file_mode == 'a':\n",
    "        if not os.path.exists(os.path.join(met.model_inferences_dir ,f'AID-{p}.h5')):\n",
    "            met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create model inference folder\n",
    "met.prepare_inference_folder\n",
    "\n",
    "# Determine the fold(s) to evaluate\n",
    "folds = met.eval_folds if met.eval_folds is not None else 0\n",
    "\n",
    "# Weight directory\n",
    "weights_dir = met.weights_dir if met.weights_dir is not None else \"\"\n",
    "\n",
    "# Sanity checks\n",
    "if type(folds) == type([]) and len(folds)>1:\n",
    "    if met.data_set_to_evaluate.lower() != 'test':\n",
    "        msg = 'Multi-fold evaluation only makes sense on the test set. Give only one fold or change the data set to test.'\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "# prepare the evaluation data\n",
    "eval_data_set = met.read_data_ids(data_type = met.data_set_to_evaluate)\n",
    "\n",
    "pat_eval = {p:{} for p in eval_data_set}  \n",
    "gt_norm_fctr, pr_norm_fctr = 1.0, 1.0\n",
    "patient_ids = tqdm(eval_data_set) if echo_progress else eval_data_set\n",
    "\n",
    "# prepare the model if only one fold is used and a prediction file is to be written\n",
    "if len(folds) == 1 and met.prediction_file_mode in ['w', 'a']: \n",
    "    model = met.prepare_model(folds[0], weights_dir, compiled=True)\n",
    "else:\n",
    "    model = None\n",
    "    \n",
    "# Evaluate the prediction for each patient in the evaluation set \n",
    "for p in patient_ids:\n",
    "    \n",
    "    # get the dose distribution for the patient\n",
    "    gt_dose, pr_dose = met.get_dose_volumes(p, folds, weights_dir, model)\n",
    "                                    \n",
    "    # Normalize the dose if desired\n",
    "    gt_dose, pr_dose, gt_norm_fctr, pr_norm_fctr = met.normalize_dose(p, gt_dose, pr_dose, \n",
    "                                                                        method=met.normalization_method, \n",
    "                                                                        normalize=met.reference_normalization_volume)\n",
    "            \n",
    "    # Save the prediction\n",
    "    if met.prediction_file_mode == 'w':\n",
    "        met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)\n",
    "    elif met.prediction_file_mode == 'a':\n",
    "        if not os.path.exists(os.path.join(met.model_inferences_dir ,f'AID-{p}.h5')):\n",
    "            met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)\n",
    "    \n",
    "    # Determine minimum dose\n",
    "    min_dose = met.get_minimum_desired_dose_for_analysis(gt_dose)\n",
    "    \n",
    "    # evaluate the prediction for patient\n",
    "    pat_eval[p] = met.evaluate_prediction(p, gt_dose, pr_dose, met.vol_for_max_dose, min_dose=min_dose)  \n",
    "                    \n",
    "# GPR analysis\n",
    "if met.include_gpr_analysis:\n",
    "    pat_eval = met.get_gamma_passing_rate(eval_data_set, pat_eval, data_folder=met.model_inferences_dir)\n",
    "\n",
    "# Save results as a pickle file\n",
    "with open(os.path.join('temp', 'data',f'evaluation_results_{\"-\".join([str(x) for x in folds])}.pickle'), 'wb') as handle:\n",
    "    pickle.dump(pat_eval, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "met.get_summay(pat_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f07cf7",
   "metadata": {},
   "source": [
    "---\n",
    "# A quick way to view the content of a dicom header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7926475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydicom import dcmread\n",
    "\n",
    "file_directory = 'write directory here'\n",
    "\n",
    "with dcmread(file_directory) as ds:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9402b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beams = list({}.keys())\n",
    "len(beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b42b3",
   "metadata": {},
   "source": [
    "---\n",
    "# Inspecting Target Dose\n",
    "\n",
    "Specify the patient ID in `PAT_ID` below and run the cell to prepare the data for inspection. \n",
    "\n",
    "You may only need to change values in the USER PARAMETERS section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6510d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, Select, HBox\n",
    "from utilities import interpolate_volume\n",
    "from dicom_toolbox import DicomToolbox\n",
    "import logging, os\n",
    "\n",
    "logger  = logging.getLogger(__name__) # create logger (this and the next 3 lines are optional)\n",
    "log_file = os.path.join('logs','preprocessing.log')\n",
    "logging.basicConfig(filename=log_file, filemode='w',\n",
    "                    level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "######### USER PARAMETERS ##############\n",
    "PAT_ID = 2153559\n",
    "CONFIG_FILE_DIR = \"configuration_files/user_config.json\"\n",
    "TARGET_TYPE = 'ptv'\n",
    "CT_MIN_VALUE = -300 # HU\n",
    "CT_MAX_VALUE = 900 # HU\n",
    "INITIAL_SLICE = None\n",
    "COLORMAP = 'jet'\n",
    "#########################################\n",
    "\n",
    "dt = DicomToolbox(CONFIG_FILE_DIR) # create an instance of the DicomToolbox class\n",
    "dt.parse_dicom_files(PAT_ID, mask_resolution='dose') # parse the dicom files\n",
    "ct = dt.ct.data # grab the ct data\n",
    "dose = dt.cumulative_dose # grab the cumulative dose data\n",
    "bn = list(dt.dose.keys())[0] # Grab the first beam name/number\n",
    "targets = [c for c in dt.contours.keys() if TARGET_TYPE in c] # Grab the target names\n",
    "structures = [c for c in dt.contours.keys() if TARGET_TYPE not in c] # Grab the structure names\n",
    "\n",
    "# Create a Select widget\n",
    "select_targets = Select(\n",
    "    options=targets,\n",
    "    value=targets[0],\n",
    "    description='Targets:',\n",
    "    rows=len(targets),\n",
    ")\n",
    "\n",
    "select_structures = Select(\n",
    "    options=structures,\n",
    "    value=structures[0],\n",
    "    description='Structures:',\n",
    "    rows=len(structures),\n",
    ")\n",
    "\n",
    "original_coordinates = (dt.ct.coordinates.z, dt.ct.coordinates.y, dt.ct.coordinates.x)\n",
    "new_coordinates = (dt.dose[bn].coordinates.z, dt.ct.coordinates.y, dt.ct.coordinates.x)\n",
    "ct = interpolate_volume(ct, original_coordinates, new_coordinates, intMethod='linear', boundError=0, fillValue=0)\n",
    "\n",
    "display(HBox([select_targets, select_structures]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_MIN_VALUE = -300 # HU\n",
    "CT_MAX_VALUE = 900 # HU\n",
    "# balance the contrast of the CT to enhance soft tissue\n",
    "ct[ct<CT_MIN_VALUE] = CT_MIN_VALUE\n",
    "ct[ct>CT_MAX_VALUE] = CT_MAX_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99773dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### USER PARAMETERS ##############\n",
    "\n",
    "MIN_DOSE = 0 # minimum dose to display in Gy\n",
    "\n",
    "#########################################\n",
    "\n",
    "plt.clf()  # Clear open figures\n",
    "fig, ax = plt.subplots(figsize=(7,7))   \n",
    "plt.tight_layout()\n",
    "\n",
    "dose_extent = [dt.dose[bn].coordinates.x.min(), dt.dose[bn].coordinates.x.max(), dt.dose[bn].coordinates.y.max(), dt.dose[bn].coordinates.y.min()]\n",
    "ct_extent = [dt.ct.coordinates.x.min(), dt.ct.coordinates.x.max(), dt.ct.coordinates.y.max(), dt.ct.coordinates.y.min()]\n",
    "slice_idx = INITIAL_SLICE if INITIAL_SLICE is not None else ct.shape[0]//2\n",
    "\n",
    "# balance the contrast of the CT to enhance soft tissue\n",
    "# ct[ct<CT_MIN_VALUE] = CT_MIN_VALUE\n",
    "# ct[ct>CT_MAX_VALUE] = CT_MAX_VALUE\n",
    "\n",
    "# coordinates\n",
    "x, y = dt.dose[bn].coordinates.x, dt.dose[bn].coordinates.y\n",
    "\n",
    "# Plot initial slice\n",
    "ct_slice = ax.imshow(ct[slice_idx], cmap='gray', interpolation='none', extent=ct_extent)\n",
    "# target = dt.contours[select_targets.value].data\n",
    "structure = dt.contours[select_structures.value].data\n",
    "dose_sn = np.ma.masked_where(dose[slice_idx] <= MIN_DOSE, dose[slice_idx])\n",
    "# dose_slice = ax.imshow(dose_sn, cmap=COLORMAP, alpha=0.3, interpolation='none', vmin=np.min(dose), vmax=np.max(dose), extent=dose_extent)\n",
    "# target_contour = ax.contour(x,y, target[slice_idx], levels=[0.5], colors='yellow')\n",
    "structure_contour = ax.contour(x,y, structure[slice_idx], levels=[0.5], colors='red')\n",
    "ax.axis('off')\n",
    "\n",
    "# Update function for scroll bar\n",
    "def update(slice_idx):\n",
    "    global target_contour, structure_contour\n",
    "    \n",
    "    ct_slice.set_data(ct[slice_idx])\n",
    "    # dose_sn = np.ma.masked_where(dose[slice_idx] < MIN_DOSE, dose[slice_idx])\n",
    "    # dose_slice.set_data(dose_sn)\n",
    "    ct_slice.set_clim(vmin=np.min(ct[slice_idx]), vmax=np.max(ct[slice_idx]))\n",
    "    # Remove previous contour lines for target\n",
    "    # for coll in target_contour.collections:\n",
    "    #     coll.remove()\n",
    " \n",
    "    # # Remove previous contour lines for structure\n",
    "    for coll in structure_contour.collections:\n",
    "        coll.remove()\n",
    " \n",
    "    # Plot new contour for target\n",
    "    \n",
    "    # target_contour = ax.contour(x, y, target[slice_idx], levels=[0.5], colors='yellow')\n",
    "    # # Plot new contour for structure\n",
    "    structure_contour = ax.contour(x, y, structure[slice_idx], levels=[0.5], colors='red')\n",
    " \n",
    "    plt.draw()\n",
    "\n",
    "# Create interactive scroll bar\n",
    "interact(update, slice_idx=IntSlider(min=0, max=ct.shape[0], step=1, value=ct.shape[0]//2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edff5b",
   "metadata": {},
   "source": [
    "-------\n",
    "# Visualize the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import h5py\n",
    "\n",
    "PAT_ID = '663560'\n",
    "MIN_DOSE = 0.03\n",
    "OAR_1 = 'liver'\n",
    "# OAR_2 = 'body'\n",
    "FILE_DIR = 'temp/data/preprocessed.h5'\n",
    "\n",
    "# close any open figures\n",
    "plt.close('all')\n",
    "\n",
    "# Initialize Plot\n",
    "\n",
    "# Initial slice\n",
    "slice_idx = 0\n",
    "\n",
    "with h5py.File(FILE_DIR, 'r') as f:\n",
    "    pat_data = f[str(PAT_ID)]\n",
    "    print(pat_data['contours'].keys())\n",
    "    ct = pat_data['ct'][:]\n",
    "    dose = pat_data['dose'][:]\n",
    "    oar_1_data = pat_data[f'contours/{OAR_1}'][:]\n",
    "    # oar_2_data = pat_data[f'contours/{OAR_2}'][:]\n",
    "    cmb_targets = pat_data['contours/combined_targets'][:]\n",
    "    max_slice = ct.shape[0]\n",
    "\n",
    "print(ct.max(), dose.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# print maximum value of the dose, ct\n",
    "print(f\"Maximum dose: {np.max(dose):.2f} Gy\")\n",
    "print(f\"Maximum CT: {np.max(ct):.2f} HU\")    \n",
    "\n",
    "# balance the contrast of the CT to enhance soft tissue\n",
    "# ct[ct<0.05] = 0.05 \n",
    "# ct[ct>.2] = .2\n",
    "    \n",
    "# Plot initial slice\n",
    "ct_slice = ax.imshow(ct[slice_idx], cmap='gray', interpolation='none')\n",
    "\n",
    "# dose_sn = np.ma.masked_where(dose[slice_idx] == 0, dose[slice_idx])\n",
    "# dose_slice = ax.imshow(dose_sn, cmap='jet', alpha=0.3, interpolation='none', vmin=np.min(dose), vmax=np.max(dose))\n",
    "oar_1_contour_plot = ax.contour(oar_1_data[slice_idx], levels=[0.5], colors='y')\n",
    "# oar_2_contour_plot = ax.contour(oar_2_data[slice_idx], levels=[0.5], colors='w')\n",
    "target_contour_plot = ax.contour(cmb_targets[slice_idx], levels=[0.5], colors='r')\n",
    "ax.axis('off')\n",
    "\n",
    "# Update function for scroll bar\n",
    "def update(slice_idx):\n",
    "\n",
    "    ct_slice.set_data(ct[slice_idx])\n",
    "    # dose_sn = np.ma.masked_where(dose[slice_idx] == 0, dose[slice_idx])\n",
    "    # dose_slice.set_data(dose_sn)\n",
    "    \n",
    "    # dose_slice.set_clim(vmin=np.min(dose[slice_idx]), vmax=np.max(dose[slice_idx]))\n",
    "    ct_slice.set_clim(vmin=np.min(ct[slice_idx]), vmax=np.max(ct[slice_idx]))\n",
    "    \n",
    "    # # Remove previous contour lines\n",
    "    for coll in oar_1_contour_plot.collections:\n",
    "        coll.remove()\n",
    "    \n",
    "    # for coll in oar_2_contour_plot.collections:\n",
    "    #     coll.remove()\n",
    "    \n",
    "    for coll in target_contour_plot.collections:\n",
    "        coll.remove()\n",
    "\n",
    "    # # Plot new contour\n",
    "    new_contour = ax.contour(oar_1_data[slice_idx], levels=[0.5], colors='y')\n",
    "    oar_1_contour_plot.collections = new_contour.collections\n",
    "    \n",
    "    # new_contour = ax.contour(oar_2_data[slice_idx], levels=[0.5], colors='w')\n",
    "    # oar_2_contour_plot.collections = new_contour.collections\n",
    "    \n",
    "    \n",
    "        \n",
    "    # # Plot new contour\n",
    "    new_contour = ax.contour(cmb_targets[slice_idx], levels=1, colors='r')\n",
    "    target_contour_plot.collections = new_contour.collections\n",
    "    \n",
    "    plt.draw()\n",
    "\n",
    "# Create interactive scroll bar\n",
    "interact(update, slice_idx=IntSlider(min=0, max=ct.shape[0], step=1, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_toolbox import ModelEvaluationTools\n",
    "from tqdm import tqdm\n",
    "import h5py, pickle, os, logging, datetime\n",
    "import multiprocessing\n",
    "\n",
    "user_inputs_dir = \"configuration_files/user_config.json\"\n",
    "echo_progress = True\n",
    "\n",
    "met = ModelEvaluationTools(user_inputs_dir)\n",
    "\n",
    "# create model inference folder\n",
    "met.prepare_inference_folder\n",
    "\n",
    "# Determine the fold(s) to evaluate\n",
    "folds = [0]\n",
    "\n",
    "# Weight directory\n",
    "weights_dir = \"\"\n",
    "\n",
    "\n",
    "# prepare the evaluation data\n",
    "eval_data_set = met.read_data_ids(data_type = met.data_set_to_evaluate)\n",
    "gt_norm_fctr, pr_norm_fctr = 1.0, 1.0\n",
    "patient_ids = tqdm(eval_data_set) if echo_progress else eval_data_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prepare the model if only one fold is used and a prediction file is to be written\n",
    "if len(folds) == 1 and met.prediction_file_mode in ['w', 'a']: \n",
    "    model = met.prepare_model(folds[0], weights_dir, compiled=True)\n",
    "else:\n",
    "    model = None\n",
    "    \n",
    "# Evaluate the prediction for each patient in the evaluation set \n",
    "for p in patient_ids:\n",
    "    \n",
    "    # get the dose distribution for the patient\n",
    "    gt_dose, pr_dose = met.get_dose_volumes(p, folds, weights_dir, model)\n",
    "                                    \n",
    "    # Normalize the dose if desired\n",
    "    gt_dose, pr_dose, gt_norm_fctr, pr_norm_fctr = met.normalize_dose(p, gt_dose, pr_dose, \n",
    "                                                                        method=met.normalization_method, \n",
    "                                                                        normalize=met.reference_normalization_volume)\n",
    "            \n",
    "    # Save the prediction\n",
    "    if met.prediction_file_mode == 'w':\n",
    "        met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)\n",
    "    elif met.prediction_file_mode == 'a':\n",
    "        if not os.path.exists(os.path.join(met.model_inferences_dir ,f'AID-{p}.h5')):\n",
    "            met.save_patient_data(p, gt_dose, pr_dose, gt_norm_factor=gt_norm_fctr, pr_norm_factor=pr_norm_fctr)\n",
    "    \n",
    "#     # Determine minimum dose\n",
    "#     min_dose = met.get_minimum_desired_dose_for_analysis(gt_dose)\n",
    "    \n",
    "#     # evaluate the prediction for patient\n",
    "#     pat_eval[p] = met.evaluate_prediction(p, gt_dose, pr_dose, met.vol_for_max_dose, min_dose=min_dose)  \n",
    "                    \n",
    "# # GPR analysis\n",
    "# if met.include_gpr_analysis:\n",
    "#     pat_eval = met.get_gamma_passing_rate(eval_data_set, pat_eval, data_folder=met.model_inferences_dir)\n",
    "\n",
    "# # Save results as a pickle file\n",
    "# with open(os.path.join('temp', 'data',f'evaluation_results_{\"-\".join([str(x) for x in folds])}.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(pat_eval, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "# met.get_summay(pat_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae38fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd220833f7b03d88d30dabcf44a18b958edf12e18c01ff0b4234ed76ee3d8afc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
