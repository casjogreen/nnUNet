{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydicom import dcmread\n",
    "from scipy.interpolate import interp1d\n",
    "from skimage.draw import polygon\n",
    "from dataclasses import dataclass, field\n",
    "from utilities import interpolate_volume, natural_keys\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "import os, json, copy, pandas, h5py, logging, re, pickle, traceback, statistics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Known issues:\n",
    "# 1. Radiation type is expected. Make this optional.\n",
    "\n",
    "@dataclass\n",
    "class Coordinates():\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    z: np.ndarray\n",
    "    dx: float\n",
    "    dy: float\n",
    "    dz: float\n",
    "    image_position: np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class CT():\n",
    "    shape: tuple  \n",
    "    resolution: np.ndarray\n",
    "    max_value: float\n",
    "    min_value: float  \n",
    "    units: str \n",
    "    rescale_slope: float\n",
    "    rescale_intercept: float\n",
    "    patient_position: str\n",
    "    data: np.ndarray\n",
    "    slice_thickness: float\n",
    "    coordinates: Coordinates\n",
    "    \n",
    "@dataclass\n",
    "class CommonDoseTags():\n",
    "    shape: tuple\n",
    "    max_value: float\n",
    "    min_value: float\n",
    "    resolution: np.ndarray\n",
    "    dose_grid_scaling: float\n",
    "    dose_units: str\n",
    "    data: np.ndarray\n",
    "    coordinates: Coordinates\n",
    "    dose_summation_type: str = 'not_specified'\n",
    "    beam_number: int = 1\n",
    "    beam_type: str = 'not_specified'\n",
    "    gantry_angle: float = 0.0\n",
    "    patient_support_angle: float = 0.0\n",
    "    table_top_pitch_angle: float = 0.0\n",
    "    table_top_roll_angle: float = 0.0\n",
    "    isocenter: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    radiation_type: str = 'not_specified'\n",
    "    treatment_delivery_type: str = 'not_specified'\n",
    "    beam_name: str = 'not_specified'\n",
    "    treatment_machine: str = 'not_specified'\n",
    "    beam_description: str = 'not_specified'\n",
    "    number_of_control_points: int = 0\n",
    "    final_cumulative_meterset_weight: float = 0.0\n",
    "    beam_dose: float = 0.0\n",
    "    scan_mode: str = 'not_specified'\n",
    "    primary_dosimetric_units: str = 'not_specified'\n",
    "    \n",
    "     \n",
    "@dataclass\n",
    "class ProtonDose(CommonDoseTags):\n",
    "    vsad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    \n",
    "@dataclass\n",
    "class PhotonDose(CommonDoseTags):\n",
    "    sad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    gantry_rotation_direction: list = field(default_factory=lambda: ['not_specified'])\n",
    "    \n",
    "@dataclass\n",
    "class Mask():\n",
    "    data: np.ndarray\n",
    "    resolution: str\n",
    "    coordinates:Coordinates\n",
    "    number: int\n",
    "    color: str = 'not_specified'\n",
    "    roi_genration_algorithm: str = 'not_specified'\n",
    "    \n",
    "@dataclass\n",
    "class Plan():\n",
    "    number_of_beams: int = 1\n",
    "    geometry: str = 'not_specified'\n",
    "    patient_position: str = 'not_specified'\n",
    "    patient_sex: str = 'not_specified'\n",
    "    plan_label: str = 'not_specified'\n",
    "    number_of_fractions_planned: int = 0\n",
    "    dose_per_fraction: float = 0.0\n",
    "    dose_reference_type: str = 'not_specified'\n",
    "    dose_reference_description: str = 'not_specified'\n",
    "    dose_reference_dose: float = 0.0\n",
    "    radiation_type: str = 'not_specified'\n",
    "    beam: dict = field(default_factory=lambda: {})\n",
    "    \n",
    "@dataclass\n",
    "class Beam():\n",
    "    gantry_angle: float = 0.0\n",
    "    patient_support_angle: float = 0.0\n",
    "    isocenter: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "    treatment_delivery_type: str = 'not_specified'\n",
    "    treatment_machine: str = 'not_specified'\n",
    "    type: str = 'not_specified'\n",
    "    sad: float = 0.0\n",
    "    vsad: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0]))\n",
    "\n",
    "class DicomTools():\n",
    "    \"\"\"\"Class for parsing a set of DICOM-RT files for a patient.\n",
    "\n",
    "        Created on the Fall of 2021 by Ivan Vazquez in collaboration with Ming Yang. \n",
    "\n",
    "        Last updated: Summer 2024\n",
    "\n",
    "        Copyright 2021-2024 Ivan Vazquez\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_inputs_dir=None, patient_data_directory=None, lut_directory=None) -> None:\n",
    "        \n",
    "        self.__logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Prepare RSP LUT directory\n",
    "        if lut_directory is not None:\n",
    "            lut_directory = os.path.join('resources','LUT')\n",
    "            if not os.path.isdir(lut_directory): \n",
    "                self.__logger.warning(f'The directory {lut_directory} for the look-up tables does not exist.')\n",
    "                self.lut_directory = None\n",
    "            else:\n",
    "                self.lut_directory = lut_directory\n",
    "\n",
    "        # Load user input information if a directory is provided\n",
    "        if user_inputs_dir is not None:\n",
    "            with open(user_inputs_dir, \"r\") as f:\n",
    "                self.user_inputs = json.load(f)  \n",
    "                self.patient_data_directory = self.user_inputs['DIRECTORIES']['raw_patient_data']\n",
    "        else:\n",
    "            self.__set_default_user_inputs()\n",
    "            \n",
    "        # set the directory for the patient data to the specified value if one is given\n",
    "        if patient_data_directory is not None:\n",
    "            assert os.path.isdir(patient_data_directory), f\"The directory {patient_data_directory} does not exist.\"\n",
    "            self.patient_data_directory = patient_data_directory \n",
    "            self.user_inputs['DIRECTORIES']['raw_patient_data'] = patient_data_directory \n",
    "\n",
    "        # Check if the necessary directories exists\n",
    "        for directory in ['logs', 'temp', os.path.join('temp','data')]:\n",
    "            if not os.path.isdir(directory): os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "        # Initialize variables\n",
    "        self.reset()\n",
    "        self.parallelize = None\n",
    "        self.n_threads = self.user_inputs[\"PARALLELIZATION\"][\"number_of_processors\"]\n",
    "        self.min_coordinate_precision = 3\n",
    "        self.expected_data = ['ct', 'rtdose', 'rtplan', 'rtstruct']\n",
    "    \n",
    "    def __set_default_user_inputs(self):\n",
    "            self.user_inputs = {\n",
    "                \"DIRECTORIES\": {\n",
    "                    \"raw_patient_data\": None,\n",
    "                    \"preprocessed_patient_data\": None,\n",
    "                    \"patient_info\": None,\n",
    "                    \"data_split\": None,\n",
    "                    \"model_weights\": None,\n",
    "                    \"model_inference\": None\n",
    "                },\n",
    "                \"TYPE_OF_TARGET_VOLUME\": \"ctv\",\n",
    "                \"PARALLELIZATION\": {\n",
    "                    \"number_of_processors\": 1,\n",
    "                },\n",
    "                \"DATA_PREPROCESSING\": {\n",
    "                    \"oars\": {\n",
    "                        \"contour_set\": \"clinical\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            self.patient_data_directory = None\n",
    "            self.__logger.warning(\"No user inputs were provided. Setting default values.\")\n",
    "            \n",
    "    def reset(self):\n",
    "        \n",
    "        self.patient_id = None\n",
    "        self.original_ct_coordinates = None\n",
    "        self.original_dose_coordinates = None\n",
    "        self.mask_interpolation_technique = 'nearest'\n",
    "        self.mask_generation_method = 'interpolate'\n",
    "        self.write_new_hdf5_file = True\n",
    "        self.relevant_masks = None\n",
    "        self.compression = 'lzf'\n",
    "        self.radiation_type = None\n",
    "        self.echo_progress = True\n",
    "        self.uniform_slice_thickness = True\n",
    "        self.echo_level = 0\n",
    "        self.minimum_ct_value = None\n",
    "        self.coordinate_precision = 3\n",
    "        self.equalize_dose_grid_dimensions = True\n",
    "        if hasattr(self, 'dicom_files'): del self.dicom_files\n",
    "          \n",
    "    def identify_patient_files(self, patient_data_directory = None, echo=False):\n",
    "        \"\"\"Function to identify the number of patient folders with all DICOM-RT files \n",
    "           needed for proper functioning of the code.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "\n",
    "            `patient_data_directory` : str\n",
    "                The location of the patient data folders containing the required DICOM-RT files.\n",
    "\n",
    "            `echo` : bool\n",
    "                Flag to prompt funtion to write the number of patient folders found\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "\n",
    "            `list`\n",
    "                Names of folders for the patients found.\n",
    "        \"\"\"\n",
    "\n",
    "        # check if a directory was specified\n",
    "        if patient_data_directory is not None: self.patient_folders_directory = patient_data_directory\n",
    "\n",
    "        # get a list of all of the folders in the directory\n",
    "        folders = os.listdir(self.patient_data_directory)\n",
    "\n",
    "        # check folder content to avoid future errors\n",
    "        patient_files_info = {f:{'modalities':[], 'folder_directory':''} for f in folders}\n",
    "        \n",
    "        self.__logger.info(f'Checking the content of {self.patient_data_directory} to identify patient folders with the required DICOM-RT files.')\n",
    "        \n",
    "        for folder in folders:\n",
    "\n",
    "            patient_folder_directory = os.path.join(self.patient_data_directory, folder)\n",
    "            \n",
    "            for root, _, files in os.walk(patient_folder_directory):\n",
    "                \n",
    "                for file in files:\n",
    "        \n",
    "                    # grab modality for DICOM file\n",
    "                    file_directory = os.path.join(root, file)\n",
    "                    \n",
    "                    try:\n",
    "                        ds = dcmread(file_directory)\n",
    "\n",
    "                        modality = ds.data_element('Modality').value\n",
    "                                                \n",
    "                        patient_files_info[folder]['modalities'].append(modality.lower())\n",
    "                                                \n",
    "                    except:\n",
    "                        self.__logger.warning(f\"The content for the folder '{folder}' could not be read\")\n",
    "                        break\n",
    "            \n",
    "            # remove repeated modality values and sort the resulting list                                        \n",
    "            patient_files_info[folder]['modalities'] = sorted(list(set(patient_files_info[folder]['modalities'])))\n",
    "            patient_files_info[folder]['folder_directory'] = patient_folder_directory\n",
    "         \n",
    "        # Record the data folders with the required DICOM-RT files\n",
    "        patients = []\n",
    "        for p in patient_files_info.keys():\n",
    "            if not all(m in patient_files_info[p]['modalities'] for m in self.expected_data):\n",
    "                self.__logger.warning(f\"The folder '{p}' is missing one or more of the required DICOM-RT files. \"\n",
    "                                      f\"Current modalities: {', '.join(patient_files_info[p]['modalities'])}\")                           \n",
    "            else:\n",
    "                patients.append(p)\n",
    "        \n",
    "        patients.sort(key=natural_keys)\n",
    "        \n",
    "        if echo: self.__logger.info(f'Found {len(patients)} patient folders in {patient_data_directory} with the required DICOM-RT files.')\n",
    "\n",
    "        return patients\n",
    "    \n",
    "    def get_header_info(self, patient_id, file_type, save_to_file=False, echo=False):\n",
    "          \n",
    "        patient_files = self.run_initial_check(patient_id)\n",
    "        \n",
    "        if file_type == 'ct':\n",
    "            files = patient_files['ct']\n",
    "        elif file_type == 'dose':\n",
    "            files = patient_files['dose']\n",
    "        elif file_type == 'plan':\n",
    "            files = patient_files['plan']\n",
    "        elif file_type == 'structures':\n",
    "            files = patient_files['structures']\n",
    "        else:\n",
    "            self.__logger.error(f'Invalid file type {file_type}.')\n",
    "            return\n",
    "        \n",
    "        for n, f in enumerate(files):\n",
    "            ds = dcmread(f)    \n",
    "            \n",
    "            if echo: print(ds) # print the header information\n",
    "            \n",
    "            # save pretty json to file in log directory\n",
    "            if save_to_file:\n",
    "                header_output_dir = os.path.join('logs', f'{file_type}_header_info_{file_type}_{n}.txt')\n",
    "                with open(header_output_dir, 'w') as outfile:\n",
    "                    print(ds, file=outfile)\n",
    "            \n",
    "            # if CT, exit\n",
    "            if file_type == 'ct': break\n",
    "                        \n",
    "    def identify_radiation_type(self, patient_files, patient_id):\n",
    "        \"\"\"Function to identify the main type of radiation therapy used for a patient. The \n",
    "        function determines the most common radiation type used for the beams in the plan file.\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        if self.radiation_type is not None: return\n",
    "        \n",
    "        if patient_files['plan'] == [] and 'rtdose' not in self.expected_data: return\n",
    "                        \n",
    "        try:      \n",
    "            ds = dcmread(patient_files['plan'][0])      \n",
    "            radiation_types = [b.RadiationType.lower() for b in ds.BeamSequence]\n",
    "            unique_radiation_types = list(set(radiation_types))\n",
    "            self.radiation_type = max(unique_radiation_types, key=radiation_types.count) \n",
    "        except:\n",
    "            try:\n",
    "                radiation_types = [b.RadiationType.lower() for b in ds.IonBeamSequence]\n",
    "                unique_radiation_types = list(set(radiation_types))\n",
    "                self.radiation_type = max(unique_radiation_types, key=radiation_types.count)                \n",
    "            except:\n",
    "                self.__logger.error(f'Failed to identify the radiation type for pat-{patient_id}.')\n",
    "                self.__logger.info('If you know the radiation type, please specify it with the class attribute \"radiation_type\".')   \n",
    "                                                        \n",
    "    def run_initial_check(self, patient_id=None):\n",
    "        \n",
    "        assert self.patient_data_directory is not None\n",
    "        if patient_id is not None:\n",
    "            if type(patient_id) != type(\"\"): patient_id = str(patient_id)\n",
    "            self.patient_id = patient_id\n",
    "        \n",
    "        # prepare patient data directory\n",
    "        patient_directory = os.path.join(self.patient_data_directory, self.patient_id)\n",
    "        \n",
    "        # Detect all files in the directory\n",
    "        try:\n",
    "            files = os.listdir(patient_directory)\n",
    "        \n",
    "            # Find directory of all type of DICOM files \n",
    "            self.dicom_files = {'ct':[], 'plan':[], 'structures':[], 'dose':[]}\n",
    "            \n",
    "            # Discover all of the files for the patients\n",
    "            for root, _, files in os.walk(patient_directory):\n",
    "                for f in files:\n",
    "                    # Get modality for the file\n",
    "                    ds = dcmread(os.path.join(root,f))\n",
    "                    modality = ds.data_element('Modality').value.lower()\n",
    "                    \n",
    "                    # Add file to the corresponding list\n",
    "                    if modality == 'rtdose':\n",
    "                        print(os.path.join(root,f))\n",
    "                        self.dicom_files['dose'].append(os.path.join(root,f))\n",
    "        \n",
    "                    elif modality == 'rtplan':\n",
    "                        self.dicom_files['plan'].append(os.path.join(root,f))\n",
    "                        \n",
    "                    elif modality == 'rtstruct':\n",
    "                        self.dicom_files['structures'].append(os.path.join(root,f)) \n",
    "                        self.dicom_files['structures'] = sorted(self.dicom_files['structures'])\n",
    "                        \n",
    "                    elif modality == 'ct':\n",
    "                        self.dicom_files['ct'].append(os.path.join(root,f))     \n",
    "                        \n",
    "        except Exception as e:\n",
    "            self.__logger.error(f\"An error occured while trying to read the files for patient {self.patient_id}.\")\n",
    "            self.__logger.error(traceback.format_exc())\n",
    "            raise Exception(f\"An error occured while trying to read the files for patient {self.patient_id}.\")\n",
    "        \n",
    "        # ensure that only one dose file has summation type of 'plan'\n",
    "        summation_types = [dcmread(f).DoseSummationType.lower() for f in self.dicom_files['dose']]\n",
    "        if summation_types.count('plan') > 1:\n",
    "            self.__logger.error(f'Multiple dose files with summation type \"plan\" were identified for patient {self.patient_id}.')\n",
    "            raise ValueError()\n",
    "                \n",
    "        # identify the radiation type\n",
    "        if self.radiation_type is None and self.dicom_files['plan'] != []: \n",
    "            try:                \n",
    "                self.identify_radiation_type(self.dicom_files, self.patient_id)\n",
    "            except:\n",
    "                self.__logger.error(f'Failed to identify the radiation type for pat-{self.patient_id}.')\n",
    "                self.radiation_type = 'not-specified'\n",
    "    \n",
    "        # Detect incomplete data\n",
    "        file_types_dict = {'ct':'ct', 'rtdose':'dose', 'rtplan':'plan', 'rtstruct':'structures'}\n",
    "        if any([self.dicom_files[k]==[] for k in [file_types_dict[x] for x in self.expected_data]]):\n",
    "            self.__logger.error(f'The full DICOM-RT set ({\", \".join(self.expected_data)}) for patient {self.patient_id} could not be read.')\n",
    "            self.__logger.info(\"Please change the expected data types by specifying the class attribute 'expected_data' or check the patient folder.\")\n",
    "            return None\n",
    "        \n",
    "        # Check the dose files to check for beam-specific or cumulative dose        \n",
    "        self.dicom_files['dose'] = self.__check_dose_files(self.dicom_files['dose'])\n",
    "            \n",
    "    def __check_dose_files(self, dose_files):\n",
    "                \n",
    "        dose_file_info = {n:{} for n in dose_files}\n",
    "        \n",
    "        for f in dose_files:\n",
    "            with dcmread(f) as ds:\n",
    "                dose_file_info[f]['dose_summation_type'] = ds.DoseSummationType.lower()\n",
    "                dose_file_info[f]['data'] = ds.pixel_array * ds.DoseGridScaling\n",
    "                try:\n",
    "                    dose_file_info[f]['beam_number'] = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                except:\n",
    "                    dose_file_info[f]['beam_number'] = 'not_specified'\n",
    "        \n",
    "        # check the dose summation type\n",
    "        if len(set([dose_file_info[f]['dose_summation_type'] for f in dose_files])) > 1:\n",
    "            self.__logger.info(f'Multiple dose summation types were identified for patient {self.patient_id}: {\", \".join([dose_file_info[f][\"dose_summation_type\"] for f in dose_files])}')\n",
    "\n",
    "            # check total dose \n",
    "            cum_dose = np.sum([dose_file_info[f]['data'] for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'beam'], axis=0)\n",
    "            # grab the dose data for file with plan as dose summation type\n",
    "            plan_dose = [dose_file_info[f]['data'] for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'plan'][0]\n",
    "            \n",
    "            if np.round(np.abs(np.subtract(cum_dose,plan_dose).max())) > 0.0:\n",
    "                self.__logger.error(f'The sum of the beam dose for patient {self.patient_id} does not match the dose for the plan file.')\n",
    "                raise ValueError()\n",
    "            else:\n",
    "                return [f for f in dose_files if dose_file_info[f]['dose_summation_type'] == 'beam']\n",
    "            \n",
    "        else:\n",
    "            # return the dose files\n",
    "            return dose_files\n",
    "                        \n",
    "    def parse_dicom_files(self, patient_id=None, mask_names_only=False, mask_resolution = None, patient_data_directory = None):\n",
    "        \n",
    "        if patient_data_directory is not None: \n",
    "            self.user_inputs['DIRECTORIES']['raw_patient_data'] = patient_data_directory\n",
    "        elif self.user_inputs['DIRECTORIES']['raw_patient_data'] is not None:\n",
    "            patient_data_directory = self.user_inputs['DIRECTORIES']['raw_patient_data'] \n",
    "        else:\n",
    "            self.__logger.error('No patient data directory was specified. Provide a directory or update the user inputs.')\n",
    "            raise FileExistsError('Missing patient data directory.')\n",
    "    \n",
    "        # initial checks\n",
    "        if patient_id is not None: \n",
    "            if type(patient_id) != type(''): patient_id = str(patient_id)\n",
    "            self.patient_id = patient_id\n",
    "        if patient_id is None and self.patient_id is None:\n",
    "            self.__logger.error('Calling DICOM parsing function without specifying a patient ID. Please provide a patient ID or update the class attribute \"patient_id\".')\n",
    "            raise Exception('Missing patient ID for DICOM parsing.')\n",
    "        self.mask_resolution = mask_resolution if mask_resolution is not None else 'dose'\n",
    "        \n",
    "        if patient_data_directory  is not None and patient_data_directory .split('.')[-1] in ['h5', 'hdf5']:\n",
    "            self.read_data_from_hdf5()\n",
    "            return\n",
    "        \n",
    "        # Identify the file types in the patient folder and type of radiation therapy\n",
    "        self.run_initial_check(self.patient_id)\n",
    "                        \n",
    "        # Parse the CT volume\n",
    "        self.ct = self.parse_ct_study_files(self.dicom_files['ct'])\n",
    "\n",
    "        # Parse the dose volume and (optionally) the plan \n",
    "        if 'plan' in self.dicom_files.keys() and self.dicom_files['plan'] != [] and self.dicom_files['dose'] != []:\n",
    "            self.dose, self.plan = self.parse_rt_dose_files(self.dicom_files['dose'], self.dicom_files['plan'])\n",
    "        elif self.dicom_files['dose'] != []:\n",
    "            self.__logger.warning(f'No plan file was found for patient {self.patient_id}. Using default values for the plan.')\n",
    "            self.dose = self.parse_rt_dose_files(self.dicom_files['dose'])\n",
    "            self.plan = Plan()\n",
    "                                     \n",
    "        # Parse the contours\n",
    "        self.contours = self.parse_structure_files(sorted(self.dicom_files['structures']), names_only=mask_names_only, resolution=self.mask_resolution)\n",
    "\n",
    "    def parse_ct_study_files(self, files=None, patient_id = None, units='hu'):\n",
    "      \n",
    "        if patient_id is not None: \n",
    "            self.patient_id = str(patient_id)\n",
    "            files = self.run_initial_check(self.patient_id)['ct']\n",
    "        \n",
    "        if self.echo_level > 0: self.__logger.info(f'Parsing CT data for patient {self.patient_id}.')\n",
    "            \n",
    "        # Prepare CT volume\n",
    "        ct_slices = {dcmread(f).ImagePositionPatient[-1]:dcmread(f).pixel_array for f in files}\n",
    "        \n",
    "        if self.echo_level > 0: self.__logger.info(f'CT slice positions: {\", \".join([str(n) for n in sorted([i for i in ct_slices.keys()])])}')\n",
    "\n",
    "        ## Construct the z coordinate array \n",
    "        z = sorted([z_coord for z_coord in ct_slices])\n",
    "        ## Build 3D CT dataset\n",
    "        data = np.array([ct_slices[i].astype(float) for i in z])\n",
    "        ## Determine the number of slice spacings used for the CT data         \n",
    "        z_spacing = list(set(list(np.round(np.array(z[1:]) - np.array(z[0:-1]), self.coordinate_precision))))\n",
    "        z = np.round(z, self.coordinate_precision)\n",
    "        \n",
    "        # Grab data properties and check consistency\n",
    "        patient_position = [dcmread(f).PatientPosition.lower() for f in files]\n",
    "        image_position_x = [round(dcmread(f).ImagePositionPatient[0], self.coordinate_precision) for f in files]\n",
    "        image_position_y = [round(dcmread(f).ImagePositionPatient[1], self.coordinate_precision) for f in files]\n",
    "        xy_resolution = [[np.round(float(i),self.coordinate_precision) for i in dcmread(f).PixelSpacing] for f in files]\n",
    "        rescale_slope = [dcmread(f).RescaleSlope for f in files]\n",
    "        rescale_intercept = [dcmread(f).RescaleIntercept for f in files]\n",
    "        slice_thickness = [round(dcmread(f).SliceThickness, self.coordinate_precision) for f in files]\n",
    "        \n",
    "        assert len(set(patient_position)) == 1, f'Multiple patient positions in the CT files were identified for patient {self.patient_id}.'\n",
    "        assert len(set(image_position_x)) == 1 and len(set(image_position_y)) == 1, f'Multiple image positions in the CT files were identified for patient {self.patient_id}.'\n",
    "        assert len(set(rescale_intercept)) == 1, f'Multiple rescale intercepts were identified for the CT data of patient {self.patient_id}.'\n",
    "        assert len(set(rescale_slope)) == 1, f'Multiple rescale slopes were identified for the CT data of patient {self.patient_id}.'\n",
    "        assert len(set([tuple(i) for i in xy_resolution])) == 1\n",
    "        \n",
    "        image_position = [image_position_x[0], image_position_y[0]]\n",
    "        rescale_slope = rescale_slope[0]\n",
    "        rescale_intercept = rescale_intercept[0]\n",
    "        xy_resolution = xy_resolution[0]\n",
    "        patient_position = patient_position[0]\n",
    "      \n",
    "        if len(z_spacing) > 1:\n",
    "            self.__logger.warning(f\"Multiple slice thicknesses were identified for the CT data of patient {self.patient_id}: {', '.join([str(i) for i in z_spacing])} mm\") \n",
    "\n",
    "        image_position = [x for x in image_position] + [round(z[0], self.coordinate_precision)]\n",
    "              \n",
    "        ## Prepare the x and y coordinates\n",
    "        x = np.round(np.arange(data.shape[2]) * xy_resolution[0] + image_position[0], self.coordinate_precision)\n",
    "        y = np.round(np.arange(data.shape[1]) * xy_resolution[1] + image_position[1], self.coordinate_precision)\n",
    "        dx, dy = xy_resolution\n",
    "                 \n",
    "        ## Interpolate volume if multiple slice thicknesses were used\n",
    "        if len(z_spacing) > 1 and self.uniform_slice_thickness:\n",
    "            min_dz = np.round(min(z_spacing), self.coordinate_precision)\n",
    "            min_z, max_z = np.round(np.min(z), self.coordinate_precision), np.round(np.max(z), self.coordinate_precision)\n",
    "            self.__logger.warning(f\"Multiple slice thicknesses were identified for the CT data of patient {self.patient_id}: {', '.join([str(i) for i in z_spacing])} mm\")  \n",
    "            self.__logger.info(f'Interpolating CT data to achieve a uniform slice thickness of {min_dz}-mm')\n",
    "            ### define new z-coordinates\n",
    "            z_new = np.round(np.arange(min_z, max_z, min_dz), self.coordinate_precision)\n",
    "            original_coordinates = (z, y, x)\n",
    "            interpolation_coordinates = (z_new, y, x) \n",
    "            data = interpolate_volume(data, original_coordinates, interpolation_coordinates, \n",
    "                                      intMethod='linear', boundError=False, fillValue=0)\n",
    "            z = z_new\n",
    "            dz = np.round(min(z_spacing), self.coordinate_precision)\n",
    "            \n",
    "        else: \n",
    "            dz = np.array(z_spacing)\n",
    "        \n",
    "        ## Create coordinate object for CT data\n",
    "        coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "        \n",
    "        ## Convert units to HU if specified\n",
    "        if units == 'hu': data = data * rescale_slope + rescale_intercept\n",
    "        if units != 'hu': units = 'original'   \n",
    "        \n",
    "        if self.minimum_ct_value is not None:\n",
    "            data[data < self.minimum_ct_value] = self.minimum_ct_value \n",
    "\n",
    "        ## Save a copy of the original CT information to help create the masks\n",
    "        self.original_ct_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "        self.original_ct_shape = data.shape\n",
    "\n",
    "        return CT(data.shape, (dx,dy,dz), np.max(data), np.min(data), units, rescale_slope,\n",
    "                  rescale_intercept, patient_position, data, slice_thickness, coordinates)\n",
    "    \n",
    "    def get_plan_info(self, dsP):\n",
    "                    \n",
    "        plan = Plan()\n",
    "\n",
    "        plan.geometry = dsP.RTPlanGeometry.lower() if hasattr(dsP, \"RTPlanGeometry\") else 'not_specified'\n",
    "        plan.patient_sex = dsP.PatientSex.lower() if dsP.PatientSex != '' else 'not_specified'\n",
    "        plan.radiation_type = self.radiation_type.lower()\n",
    "        \n",
    "        for fgs in dsP.FractionGroupSequence:\n",
    "            plan.number_of_fractions_planned = int(fgs.NumberOfFractionsPlanned) if hasattr(fgs, \"NumberOfFractionsPlanned\") else 0\n",
    "            plan.number_of_beams = int(fgs.NumberOfBeams)\n",
    "            dose_per_beam = []\n",
    "            for rbs in fgs.ReferencedBeamSequence:\n",
    "                dose_per_beam.append(float(rbs.BeamDose) if hasattr(rbs, \"BeamDose\") else 0.0)\n",
    "                \n",
    "            plan.dose_per_fraction = np.sum(dose_per_beam) \n",
    "\n",
    "            if hasattr(fgs, \"DoseReferenceSequence\"):\n",
    "                for drs in fgs.DoseReferenceSequence:\n",
    "                    if hasattr(drs, \"DoseReferenceStructureType\") and drs.DoseReferenceStructureType.lower() == 'site':\n",
    "                        plan.dose_reference_type = drs.DoseReferenceType.lower() if hasattr(drs, \"DoseReferenceType\") else 'not_specified'\n",
    "                        plan.dose_reference_description = drs.DoseReferenceDescription.lower() if hasattr(drs, \"DoseReferenceDescription\") else 'not_specified'\n",
    "                        plan.dose_reference_dose = drs.TargetPrescriptionDose.lower() if hasattr(drs, \"TargetPrescriptionDose\") else 0.0\n",
    "            \n",
    "            plan.plan_label = dsP.RTPlanLabel.lower() if hasattr(dsP, \"RTPlanLabel\") else 'not_specified'\n",
    "\n",
    "            patient_position = list(set([x.PatientPosition for x in dsP.PatientSetupSequence]))\n",
    "            if len(patient_position) > 1:\n",
    "                self.__logger.warning(f'Multiple patient positions were identified for patient {self.patient_id}: {\", \".join(patient_position)}')\n",
    "            else:\n",
    "                plan.patient_position = patient_position[0].lower()\n",
    "        \n",
    "        information_sequence = dsP.IonBeamSequence if self.radiation_type == 'proton' else dsP.BeamSequence\n",
    "        \n",
    "        for b in information_sequence:\n",
    "            cps = b.IonControlPointSequence[0] if self.radiation_type == 'proton' else b.ControlPointSequence[0]\n",
    "            if b.TreatmentDeliveryType.lower() == 'setup': continue # skip setup beams\n",
    "            \n",
    "            plan.beam[int(b.BeamNumber)] = Beam()\n",
    "            plan.beam[int(b.BeamNumber)].type = b.BeamType.lower()\n",
    "            if hasattr(b, \"VirtualSourceAxisDistances\"):\n",
    "                plan.beam[int(b.BeamNumber)].vsad = b.VirtualSourceAxisDistances\n",
    "            else: \n",
    "                plan.beam[int(b.BeamNumber)].sad = float(b.SourceAxisDistance)\n",
    "                \n",
    "            plan.beam[int(b.BeamNumber)].gantry_angle = cps.GantryAngle\n",
    "            plan.beam[int(b.BeamNumber)].patient_support_angle = cps.PatientSupportAngle\n",
    "            plan.beam[int(b.BeamNumber)].isocenter = cps.IsocenterPosition\n",
    "            plan.beam[int(b.BeamNumber)].treatment_delivery_type = b.TreatmentDeliveryType.lower()\n",
    "            plan.beam[int(b.BeamNumber)].treatment_machine = b.TreatmentMachineName.lower()\n",
    "                    \n",
    "        return plan\n",
    "    \n",
    "    def get_additional_details_from_plan_file(self, dsP, dose, bn):\n",
    "        \n",
    "        # TODO: VMAT plans have more than one angle. This needs to be handled.\n",
    "                \n",
    "        # grab the beam dose      \n",
    "        for fgs in dsP.FractionGroupSequence:\n",
    "            for rbs in fgs.ReferencedBeamSequence:\n",
    "                if int(rbs.ReferencedBeamNumber) == bn: \n",
    "                    dose.beam_dose = rbs.BeamDose if hasattr(rbs, \"BeamDose\") else 0\n",
    "    \n",
    "        information_sequence = dsP.IonBeamSequence if self.radiation_type == 'proton' else dsP.BeamSequence\n",
    " \n",
    "        for b in information_sequence:\n",
    "            if int(b.BeamNumber) == bn:\n",
    "                \n",
    "                cps = b.IonControlPointSequence[0] if self.radiation_type == 'proton' else b.ControlPointSequence[0]\n",
    "                \n",
    "                dose.beam_type = b.BeamType.lower()\n",
    "                dose.radiation_type = b.RadiationType.lower()\n",
    "                dose.beam_name = b.BeamName.lower()\n",
    "                dose.beam_number = int(b.BeamNumber)\n",
    "                dose.beam_description = b.BeamDescription.lower() if hasattr(b, \"BeamDescription\") else 'not_specified'\n",
    "                dose.treatment_machine = b.TreatmentMachineName.lower()\n",
    "                dose.final_cumulative_meterset_weight = b.FinalCumulativeMetersetWeight\n",
    "                dose.scan_mode = b.ScanMode.lower() if hasattr(b, \"ScanMode\") else 'not_specified'\n",
    "                dose.treatment_delivery_type = b.TreatmentDeliveryType.lower()\n",
    "                dose.primary_dosimetric_units = b.PrimaryDosimeterUnit.lower()\n",
    "                dose.number_of_control_points = int(b.NumberOfControlPoints)\n",
    "                dose.gantry_angle = cps.GantryAngle\n",
    "                dose.patient_support_angle = cps.PatientSupportAngle if hasattr(cps, \"PatientSupportAngle\") else 0.0\n",
    "                dose.table_top_pitch_angle = cps.TableTopPitchAngle if hasattr(cps, \"TableTopPitchAngle\") else 0.0\n",
    "                dose.table_top_roll_angle = cps.TableTopRollAngle if hasattr(cps, \"TableTopRollAngle\") else 0.0\n",
    "                dose.isocenter = cps.IsocenterPosition\n",
    "                \n",
    "                if hasattr(b, \"VirtualSourceAxisDistances\"):\n",
    "                    dose.vsad = b.VirtualSourceAxisDistances\n",
    "                else:\n",
    "                    dose.sad = b.SourceAxisDistance\n",
    "                    dose.gantry_rotation_direction = b.GantryRotationDirection.lower() if hasattr(b, \"GantryRotationDirection\") else 'not_specified'\n",
    "            \n",
    "        return dose\n",
    "\n",
    "    def parse_rt_dose_files(self, dose_files=None, plan_file=None, patient_id = None):\n",
    "        \n",
    "        assert dose_files is not None or patient_id is not None \n",
    "        \n",
    "        if patient_id is not None: \n",
    "            self.patient_id = str(patient_id)\n",
    "            \n",
    "        if dose_files is None:\n",
    "            assert hasattr(self, 'patient_id'), 'No patient ID was specified.'\n",
    "            self.run_initial_check(self.patient_id)\n",
    "            dose_files, plan_file = self.dicom_files['dose'], self.dicom_files['plan']\n",
    "                     \n",
    "        dose, args = {}, []\n",
    "        for f in dose_files:\n",
    "            print(f)\n",
    "            with dcmread(f) as ds:                                                \n",
    "                if len(dose_files) == 1: # handles the case for just one dose file\n",
    "                    try: # check if the dose file is a beam-specific dose file\n",
    "                        bn = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                    except: # if not, assume it is a cumulative dose file\n",
    "                        bn = 1\n",
    "                    self.__logger.info(f'Only one dose file was found for patient {self.patient_id}. ' +\n",
    "                                        'Assuming that the dose file contains the cumulative dose for the plan.')\n",
    "                elif ds.DoseSummationType.lower() != 'plan': # handles the case for multiple dose files (beam-specific)\n",
    "                    bn = int(ds.ReferencedRTPlanSequence[0][('300c','0020')][0][('300c','0004')][0][('300c','0006')].value)\n",
    "                elif ds.DoseSummationType.lower() == 'plan': # handles the case for multiple dose files (cumulative)\n",
    "                    bn = 'plan'\n",
    "                                           \n",
    "                # Grab data\n",
    "                data = ds.pixel_array * ds.DoseGridScaling\n",
    "                # Grab some data properties\n",
    "                units = ds.DoseUnits\n",
    "                xy_resolution = [np.round(float(x), self.coordinate_precision) for x in ds.PixelSpacing]\n",
    "                dose_grid_scaling = float(ds.DoseGridScaling)\n",
    "                image_position = [np.round(float(i), self.coordinate_precision) for i in ds.ImagePositionPatient]\n",
    "                grid_offset_vector = np.round(np.array(ds.GridFrameOffsetVector), self.coordinate_precision)\n",
    "                summation_type = ds.DoseSummationType.lower()\n",
    "                \n",
    "                # Prepare coordinates\n",
    "                x = np.round(np.arange(ds.Columns)*xy_resolution[0] + image_position[0], self.coordinate_precision)\n",
    "                y = np.round(np.arange(ds.Rows)*xy_resolution[1] + image_position[1], self.coordinate_precision)\n",
    "                z = np.round(grid_offset_vector+ image_position[2], self.coordinate_precision)\n",
    "                ## Determine the number of slice spacings used for the dose data         \n",
    "                z_spacing = list(set(list(np.round(np.array(z[1:]) - np.array(z[0:-1]), self.coordinate_precision))))\n",
    "                ## Interpolate volume if multiple slice thicknesses were used\n",
    "                if len(z_spacing) > 1:\n",
    "                    min_dz = min(z_spacing)\n",
    "                    min_z, max_z = np.round(np.min(z), self.coordinate_precision), np.round(np.max(z), self.coordinate_precision)\n",
    "                    self.__logger.warning(f'Two or more slice thicknesses identified for the dose data of patient {self.patient_id}.')    \n",
    "                    self.__logger.info(f'Interpolating dose data to achieve a uniform slice thickness of {min_dz}-mm')\n",
    "                    ### define new z-coordinates\n",
    "                    z_new = np.round(np.arange(min_z, max_z, min_dz), self.coordinate_precision)\n",
    "                    original_coordinates = (z, y, x)\n",
    "                    interpolation_coordinates = (z_new, y, x) \n",
    "                    data = interpolate_volume(data, original_coordinates, interpolation_coordinates, \n",
    "                                              intMethod='linear', boundError=False, fillValue=0)\n",
    "                    z = z_new\n",
    "                \n",
    "                # Grab coordinate information\n",
    "                dx, dy, dz = xy_resolution + [min(z_spacing)]\n",
    "                coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "                \n",
    "                # Prepare dose object\n",
    "                if self.radiation_type == 'photon':\n",
    "                    dose[bn] = PhotonDose(*[data.shape, data.max(), data.min(), (dx,dy,dz), \n",
    "                                            dose_grid_scaling, units, data, coordinates, summation_type])\n",
    "                else:\n",
    "                    dose[bn] = ProtonDose(*[data.shape, data.max(), data.min(), (dx,dy,dz), \n",
    "                                            dose_grid_scaling, units, data, coordinates, summation_type])\n",
    "                \n",
    "                # Grab additional information from plan file if available\n",
    "                if plan_file is not None and plan_file != []:\n",
    "                    with dcmread(plan_file[0]) as dsP:                                         \n",
    "                        dose[bn] = self.get_additional_details_from_plan_file(dsP, dose[bn], bn)\n",
    "                                            \n",
    "        if len(set([dose[bn].data.shape for bn in dose.keys()])) > 1:      \n",
    "            self.__logger.warning(f'Not all of the dose volumes have the same shape for patient {self.patient_id}.')\n",
    "            if self.equalize_dose_grid_dimensions:\n",
    "                self.__logger.info('Equalizing dose grid dimensions')\n",
    "                dose = self.__equalize_dose_grid_dimensions(dose)\n",
    "\n",
    "        # Save a copy of the original dose information to help create the masks\n",
    "        assert len(set([dose[bn].data.shape for bn in dose.keys()])) == 1    \n",
    "        self.original_dose_coordinates = copy.deepcopy(dose[bn].coordinates)\n",
    "        self.original_dose_shape = data.shape\n",
    "\n",
    "        # Grab additional information from plan file if available\n",
    "        if plan_file is not None and plan_file != []: \n",
    "            with dcmread(plan_file[0]) as dsP:\n",
    "            \n",
    "                plan = self.get_plan_info(dsP)\n",
    "\n",
    "            return dose, plan\n",
    "        else:\n",
    "            return dose\n",
    "        \n",
    "    def __equalize_dose_grid_dimensions(self, dose):\n",
    "            \n",
    "        max_shape = np.max([dose[bn].data.shape for bn in dose.keys()], axis=0)\n",
    "\n",
    "        bn_to_correct = []\n",
    "\n",
    "        for k in dose.keys():\n",
    "            num_max_dims = []\n",
    "            for n,s in enumerate(dose[k].data.shape):\n",
    "                if s == max_shape[n]: \n",
    "                    num_max_dims.append(k)\n",
    "            if len(num_max_dims) == 3: \n",
    "                max_dim_bn = k\n",
    "                break\n",
    "\n",
    "        if 'max_dim_bn' not in locals(): raise ValueError('Unable to find a beam with the maximum shape along all dimensions.')\n",
    "        \n",
    "        # determine patient(s) needing correction\n",
    "        for k in dose.keys():\n",
    "            for n,s in enumerate(dose[k].data.shape):\n",
    "                if s != max_shape[n]: \n",
    "                    bn_to_correct.append(k)\n",
    "                    \n",
    "        for b in bn_to_correct:\n",
    "            original_coordinates = (dose[b].coordinates.z, dose[b].coordinates.y, dose[b].coordinates.x) \n",
    "            interpolation_coordinates = (dose[max_dim_bn].coordinates.z, dose[max_dim_bn].coordinates.y, dose[max_dim_bn].coordinates.x)   \n",
    "            data = interpolate_volume(dose[b].data, original_coordinates, interpolation_coordinates,\n",
    "                                    intMethod='linear', boundError=0, fillValue=0)\n",
    "            \n",
    "            dose[b].data = data\n",
    "            dose[b].coordinates = copy.deepcopy(dose[max_dim_bn].coordinates)\n",
    "            dose[b].shape = data.shape\n",
    "                        \n",
    "        return dose\n",
    "                       \n",
    "    def parse_structure_files(self, files = None, patient_id = None, mask_names = None, resolution = 'dose', names_only = False):\n",
    "                \n",
    "        # Fetch structure files if they are not provided\n",
    "        if files is None:\n",
    "            self.patient_id = patient_id if patient_id is not None else self.patient_id\n",
    "            assert self.patient_id is not None, 'Patient ID is missing.'\n",
    "            self.run_initial_check(self.patient_id)\n",
    "        \n",
    "        self.structure_files = self.dicom_files['structures']\n",
    "            \n",
    "        # ensure that mask names are in a list\n",
    "        if type(mask_names) != type([]) and type(mask_names) == type(''): mask_names = [mask_names]\n",
    "        \n",
    "        # Find the name of all of the masks in the plan\n",
    "        self.all_mask_names = []\n",
    "        for s_file in self.structure_files:                       \n",
    "            with dcmread(s_file) as ds:\n",
    "                self.all_mask_names += list(self.read_structure(ds).keys())\n",
    "        \n",
    "        # return all available masks names only: avoids building a volume\n",
    "        if names_only: return self.all_mask_names\n",
    "\n",
    "        # grab ct information if the CT study has not been read yet\n",
    "        if self.original_ct_coordinates is None:\n",
    "            self.parse_ct_study_files(patient_id=self.patient_id)\n",
    "\n",
    "        if self.original_dose_coordinates is None and resolution.lower() == 'dose':\n",
    "            self.parse_rt_dose_files(patient_id = self.patient_id)\n",
    "\n",
    "        # Grab the coordinates for the contours\n",
    "        coordinates = copy.deepcopy(self.original_ct_coordinates) if resolution.lower() == 'ct' else copy.deepcopy(self.original_dose_coordinates)\n",
    "    \n",
    "        # Ensure that only the relevant masks for the patient are parsed\n",
    "        if self.relevant_masks is not None and mask_names is None:\n",
    "            assert all([m in self.all_mask_names for m in self.relevant_masks]), 'One or more of the specified masks is not available for the patient.'\n",
    "            self.all_mask_names = self.relevant_masks\n",
    "                   \n",
    "        # prepare the output dictionary\n",
    "        contours = {}\n",
    "        \n",
    "        for s_file in self.structure_files:\n",
    "                                \n",
    "            with dcmread(s_file) as ds: \n",
    "                                    \n",
    "                structures = self.read_structure(ds)   \n",
    "                \n",
    "                for k in structures.keys() if mask_names is None else mask_names:\n",
    "                                            \n",
    "                    # grab the data for the mask\n",
    "                    if k not in self.all_mask_names:\n",
    "                            self.__logger.warning(f'A mask \"{k}\" was not found for patient-{self.patient_id}.')\n",
    "                            data = self.__return_empty_mask(resolution)   \n",
    "                    elif k in self.all_mask_names and k not in structures.keys():\n",
    "                        continue\n",
    "                    else:\n",
    "                        data = self.get_mask(structures, k, resolution = resolution)\n",
    "                    \n",
    "                    # generate a unique name for the mask\n",
    "                    name = self.__get_unique_mask_name(k, contours.keys())\n",
    "                    \n",
    "                    # create the mask object\n",
    "                    try:\n",
    "                        contours[name] = Mask(data, resolution, coordinates, \n",
    "                                            structures[k]['number'], \n",
    "                                            structures[k]['color'],\n",
    "                                            structures[k]['generation_algorithm'])\n",
    "                    except:\n",
    "                        if self.echo_level > 0:\n",
    "                            self.__logger.error(f'Failed to build the mask \"{k}\" for pat-{self.patient_id}.')\n",
    "                            self.__logger.error(traceback.format_exc())\n",
    "                            \n",
    "                        contours[name] = Mask(self.__return_empty_mask(resolution), resolution, coordinates, 'not_specified', 'not_specified', 'not_specified')\n",
    "\n",
    "        return contours \n",
    "    \n",
    "    def __get_unique_mask_name(self, mask_name, current_mask_names):   \n",
    "        if mask_name not in current_mask_names: return mask_name \n",
    "        mask_number = 1\n",
    "        final_name = f'{mask_name}_{mask_number}'\n",
    "        while final_name in current_mask_names:\n",
    "            mask_number += 1\n",
    "            final_name = f'{mask_name}_{mask_number}'\n",
    "        return final_name\n",
    "    \n",
    "    def __return_empty_mask(self, resolution):\n",
    "        try:\n",
    "            if 'ct' in resolution:\n",
    "                return np.zeros_like(self.ct.data)\n",
    "            else:\n",
    "                bn = list(self.dose.keys())[0]\n",
    "                return np.zeros_like(self.dose[bn].data)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def read_structure(self, ds):\n",
    "        \"\"\"Auxiliary function for reading the content of the structure file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds : pydycom object\n",
    "            Handle for the file opened using the pydicom module \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary contraining the controu data, color, number, and name of \n",
    "            the contrours.\n",
    "        \"\"\"\n",
    "        contours = {}\n",
    "         \n",
    "        for i in range(len(ds.ROIContourSequence)):\n",
    "            contour = {}\n",
    "            try:\n",
    "                contour['contour_data'] = [s.ContourData for s in ds.ROIContourSequence[i].ContourSequence]\n",
    "                contour['color'] = ds.ROIContourSequence[i].ROIDisplayColor\n",
    "                contour['number'] = ds.ROIContourSequence[i].ReferencedROINumber\n",
    "                contour['generation_algorithm'] = str(ds.StructureSetROISequence[i].ROIGenerationAlgorithm).lower()\n",
    "                contours[str(ds.StructureSetROISequence[i].ROIName).lower()] = contour\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        return contours\n",
    "\n",
    "    def get_mask(self, contours, name, resolution = 'ct', method = 'interpolate'):\n",
    "        \"\"\"Function to construct a binary mask using the set of contouring coordinate\\n \n",
    "        triplets stored in the structure dicom file (RTS).\n",
    "                   \n",
    "        Created by Ivan Vazquez in collaboration with Ming Yang\n",
    "        \n",
    "        Last Update: Fall 2023\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            `contours (list)`: List containing the contouring data (coordinates), color, number, and name of the contours.\n",
    "            `name` (str): Name of specific contour to build into a mask. Defaults to None.\n",
    "            `resolution` (str, optional): Resolution of the grid used to construct the mask. It can be 'ct' or 'dose'. Defaults to 'ct'.\n",
    "            `method` (str, optional): Method used to produce volumes at the resolution of the dose grid. Defaults to 'interpolate'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            `ndarray(dtype=float, ndim=2)`: Binary mask \n",
    "        \"\"\"   \n",
    "          \n",
    "        method = self.mask_generation_method if method is None else method\n",
    "           \n",
    "        z = self.original_ct_coordinates.z[:] if method == 'interpolate' else self.original_dose_coordinates.z[:]\n",
    "        y_0 = self.original_ct_coordinates.image_position[1] if method == 'interpolate' else self.original_dose_coordinates.image_position[1]\n",
    "        dy = self.original_ct_coordinates.dy if method == 'interpolate' else self.original_dose_coordinates.dy\n",
    "        x_0 = self.original_ct_coordinates.image_position[0] if method == 'interpolate' else self.original_dose_coordinates.image_position[0]\n",
    "        dx = self.original_ct_coordinates.dx if method == 'interpolate' else self.original_dose_coordinates.dx\n",
    "        z_min, z_max = np.round(z.min(),self.coordinate_precision), np.round(z.max(),self.coordinate_precision)\n",
    "            \n",
    "        # allocate volume for mask using the shape of the original CT volume\n",
    "        shape = self.original_dose_shape if resolution == 'dose' and method != 'interpolate' else self.original_ct_shape\n",
    "        mask = np.zeros(shape, dtype=np.uint8) \n",
    "        \n",
    "        # round the z coordinates to avoid floating point errors\n",
    "        z = np.round(z,self.coordinate_precision)\n",
    "                  \n",
    "        ## Grab the contour data that matches the name of the desired mask\n",
    "        contour_data = [np.array(i).reshape(-1,3) for i in [contours[c]['contour_data'] for c in contours.keys() if c.lower() == name.lower()][0]]\n",
    "                    \n",
    "        for nodes in contour_data: \n",
    "            \n",
    "            z_node = np.round(nodes[0, 2],self.coordinate_precision)  \n",
    "                                 \n",
    "            if np.logical_and(z_node >= z_min, z_node <= z_max): # ignore slices outside of the CT volume\n",
    "            \n",
    "                z_index = np.where(z == z_node)[0][0]\n",
    "         \n",
    "                r = (nodes[:, 1] - y_0) / dy\n",
    "                c = (nodes[:, 0] - x_0) / dx \n",
    "                \n",
    "                # make values larger than max index equal to max index\n",
    "                r[np.where(r > mask.shape[1]-1)] = mask.shape[1]-1\n",
    "                c[np.where(c > mask.shape[2]-1)] = mask.shape[2]-1\n",
    "                \n",
    "                rr, cc = polygon(r, c)\n",
    "            \n",
    "                mask[z_index, rr, cc] += 1\n",
    "          \n",
    "        mask[np.where(mask>1)] = 0 # account for holes (mask ==2) in a contour\n",
    "                      \n",
    "        if resolution == 'dose' and method == 'interpolate':\n",
    "            oc = (self.original_ct_coordinates.z, self.original_ct_coordinates.y, self.original_ct_coordinates.x)\n",
    "            ic = (self.original_dose_coordinates.z, self.original_dose_coordinates.y, self.original_dose_coordinates.x)\n",
    "            return interpolate_volume(mask, oc, ic, intMethod=self.mask_interpolation_technique, boundError=0, fillValue=0)\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def convert_hu_to_rsp(self, ct=None, in_place=True, interpolation_kind = 'linear', lut_directory=None):\n",
    "        \n",
    "        # Read LUT for HU to RSP conversion\n",
    "        if lut_directory is not None: self.rsp_lut_directory = lut_directory\n",
    "        if self.rsp_lut_directory is None: \n",
    "            self.__logger.error('No LUT directory was provided for the HU to RSP conversion.')\n",
    "            raise ValueError('No LUT directory was found or provided for the HU to RSP conversion.')\n",
    "        \n",
    "        rsp_lut_dir = os.path.join(self.rsp_lut_directory, 'mda_relative_stopping_power.csv') \n",
    "        \n",
    "        try:\n",
    "            HU_2_RSP_LUT = pandas.read_csv(rsp_lut_dir)\n",
    "        except:\n",
    "            self.__logger.error(f'Error reading the HU to RSP LUT from \"{rsp_lut_dir}\". This should be a CSV file.')\n",
    "            raise ValueError(f'Error reading the HU to RSP LUT from \"{rsp_lut_dir}\".')\n",
    "        HU = HU_2_RSP_LUT['HU'].values\n",
    "        rsp = HU_2_RSP_LUT['rsp'].values\n",
    "        \n",
    "        # create interpolating function\n",
    "        rsp2Hu = interp1d(HU, rsp, kind = interpolation_kind)\n",
    "\n",
    "        if ct is None: ct = self.ct\n",
    "        data = ct.data\n",
    "\n",
    "        # Ensure that the min and max values of CT's HU values are inside acceptable range\n",
    "        badLowInds = np.where(data < HU.min())\n",
    "        data[badLowInds] = HU.min()\n",
    "\n",
    "        badHighInds = np.where(data > HU.max())\n",
    "        data[badHighInds] = HU.max()\n",
    "\n",
    "        # Use interpolating function to convert the CT array\n",
    "        rspVol = rsp2Hu(data.flatten()).reshape(ct.shape)\n",
    "\n",
    "        if in_place: \n",
    "            ct.data = rspVol\n",
    "        else:\n",
    "            return rspVol\n",
    "\n",
    "    def __get_patient_list(self, patient_data_directory):\n",
    "        if os.path.isdir(patient_data_directory):\n",
    "            return self.identify_patient_files(patient_data_directory)\n",
    "        elif os.path.isfile(patient_data_directory) and patient_data_directory.split('.')[-1] in ['h5', 'hdf5']:\n",
    "            with h5py.File(patient_data_directory, 'r') as hf:\n",
    "                return list(hf.keys())\n",
    "        else:\n",
    "            raise Exception (f\"Unable to work with the patient data directory: {patient_data_directory}\")\n",
    "\n",
    "    def get_dicom_data_report(self, patient_list=None, save = True):\n",
    "        \n",
    "        # grab list of patient files to explore\n",
    "        if patient_list is None: \n",
    "            patient_list = self.__get_patient_list(self.patient_data_directory)\n",
    "        else:\n",
    "            if type(patient_list) != type([]): patient_list = [patient_list]\n",
    "            patient_list = sorted([str(n) for n in patient_list])\n",
    "        \n",
    "        if self.parallelize is None: self.identify_parallel_capabilitie  # check if parallel processing should be used\n",
    "        \n",
    "        def get_report(my_patients, infer_rx_dose, process_id=None, save_file = True):\n",
    "            \n",
    "            data_report = {p:{} for p in my_patients}\n",
    "             \n",
    "            # check if parallel processing is possible\n",
    "            if not self.parallelize:\n",
    "                if self.echo_progress: my_patients = tqdm(my_patients, desc=\"Generating basic report\", leave=True)\n",
    "            \n",
    "            for p in my_patients: \n",
    "                \n",
    "                self.reset()\n",
    "                                            \n",
    "                try:        \n",
    "                    self.parse_dicom_files(p, mask_names_only=True) # parse the data and grab contour names only\n",
    "                    beams = list(self.plan.beam.keys())\n",
    "                    data_report[p]['number_of_beams'] = len(beams) if beams != [] else 1\n",
    "                    data_report[p]['radiation_type'] = self.radiation_type\n",
    "                    data_report[p]['gantry_angles'] = [self.plan.beam[b].gantry_angle for b in beams]\n",
    "                    data_report[p]['couch_angles'] = [self.plan.beam[b].patient_support_angle for b in beams]\n",
    "                    data_report[p]['ct_array_dimensions'] = self.ct.data.shape\n",
    "                    data_report[p]['dose_array_dimensions'] = self.dose[beams[0]].data.shape\n",
    "                    data_report[p]['dose_array_resolution'] = {'dx':self.dose[beams[0]].coordinates.dx,\n",
    "                                                               'dy':self.dose[beams[0]].coordinates.dy,\n",
    "                                                               'dz':self.dose[beams[0]].coordinates.dz}\n",
    "                    if self.radiation_type == 'proton':\n",
    "                        data_report[p]['vsad'] = [self.plan.beam[b].vsad for b in beams]\n",
    "                    else:\n",
    "                        data_report[p]['sad'] = [self.plan.beam[b].sad for b in beams]\n",
    "                        data_report[p]['gantry_rotation_direction'] = self.dose[beams[0]].gantry_rotation_direction\n",
    "                    data_report[p]['isocenter'] = [self.plan.beam[b].isocenter for b in beams]\n",
    "                    data_report[p]['contours'] = ','.join(sorted(self.contours))\n",
    "                    data_report[p]['dose_reference_dose']= self.plan.dose_reference_dose\n",
    "                    data_report[p]['dose_reference_type']= self.plan.dose_reference_type\n",
    "                    data_report[p]['dose_reference_description']= self.plan.dose_reference_description\n",
    "                    data_report[p]['beam_type'] = list(set([self.plan.beam[b].type for b in beams]))\n",
    "                    data_report[p]['patient_position'] = self.ct.patient_position\n",
    "                                       \n",
    "                except Exception as e:\n",
    "                    self.__logger.error(f\"The data for pat-{p} could not be analyzed\")\n",
    "                    self.__logger.error(traceback.format_exc())\n",
    "                    \n",
    "            if save_file:\n",
    "                tag = f'_{process_id}' if process_id is not None else ''\n",
    "                with open(os.path.join('temp','data',f'basic_data_report{tag}.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(data_report, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        \n",
    "            if not self.parallelize: return data_report\n",
    "                \n",
    "        if self.parallelize:\n",
    "            \n",
    "            self.__logger.info(f'Parallelizing the data report generation using {self.n_threads} threads.')\n",
    "                        \n",
    "            processes = [] # initialize a list to store the processes\n",
    "                \n",
    "            # divide the patients into groups based on the number of available threads\n",
    "            chunked_patients = [x for x in np.array_split(patient_list, self.n_threads) if x.size != 0]\n",
    "            \n",
    "            # create a process for each thread\n",
    "            for n, patients in enumerate(chunked_patients):\n",
    "                p = mp.Process(target=get_report, args=(patients, infer_rx_dose, n, save,))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "            \n",
    "            for p in processes:\n",
    "                p.join()   \n",
    "            \n",
    "        else:\n",
    "            data_report = get_report(patient_list, infer_rx_dose, save_file = save)\n",
    "        \n",
    "        # merge data reports and remove temp files\n",
    "        if self.parallelize and save:\n",
    "            data_report = {}\n",
    "            for n in range(len(chunked_patients)):\n",
    "                with open(os.path.join('temp','data',f'basic_data_report_{n}.pickle'), 'rb') as handle:\n",
    "                    data_report.update(pickle.load(handle))\n",
    "                os.remove(os.path.join('temp','data',f'basic_data_report_{n}.pickle'))\n",
    "            # save merged data report\n",
    "            with open(os.path.join('temp','data','basic_data_report.pickle'), 'wb') as handle:\n",
    "                pickle.dump(data_report, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                       \n",
    "        self.__logger.info(f\"Finished generating basic report for all detected patients.\")\n",
    "\n",
    "        return data_report\n",
    "    \n",
    "    def infer_rx_dose(self, patient_list=None, min_infered_rx_dose = 20, get_dose_statistics = True):\n",
    "            \n",
    "        if patient_list is None: \n",
    "            patient_list = self.__get_patient_list(self.patient_data_directory)\n",
    "        else:\n",
    "            if type(patient_list) != type([]): patient_list = [patient_list]\n",
    "            patient_list = sorted([str(n) for n in patient_list])\n",
    "            \n",
    "        target_properties = {p : {} for p in patient_list} # TODO: change this to dose information\n",
    "        rx_dose_info ={'AID':[p for p in patient_list], 'Rx Dose':[], 'Plan Type':[], 'dose_scale':[], 'site':[]}\n",
    "        \n",
    "        if self.echo_progress: patient_list = tqdm(patient_list, desc=\"Infering prescription doses:\", leave=True)\n",
    "\n",
    "        for p in patient_list: \n",
    "                \n",
    "            self.reset()\n",
    "                                        \n",
    "            self.parse_dicom_files(p, mask_names_only=True)\n",
    "            \n",
    "            # Indentify target volumes\n",
    "            all_target_volumes = [x for x in self.contours if self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"] in x]\n",
    "            len_target_name = len(self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"])\n",
    "            target_volumes = [x for x in all_target_volumes if x[:len_target_name] == self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"]]        \n",
    "            \n",
    "            # Infer the prescription dose \n",
    "            rx = [self.__find_posible_rx_dose(x) for x in target_volumes]\n",
    "            rx = list(set([str(x) for x in rx if float(x) >=min_infered_rx_dose]))    \n",
    "            rx = [x for x in rx if x != '0']\n",
    "            if len(rx) > 1: # if multiple rx doses are found, check if they are multiples of each other\n",
    "                rx = [float(x)/100 if float(x) > 100 else float(x) for x in rx]\n",
    "                rx = [str(x) for x in set(rx)]\n",
    "                \n",
    "            target_properties[p]['infered_rx_dose'] = ','.join(rx) if rx != [] else '0'\n",
    "            target_properties[p]['all_target_like_structures'] = ','.join([x for x in self.contours if self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"] in x])\n",
    "            rx_dose_info['Rx Dose'].append(','.join(rx) if rx != [] else '0')\n",
    "            rx_dose_info['Plan Type'].append('Unknown')\n",
    "            rx_dose_info['dose_scale'].append(1.0)\n",
    "            rx_dose_info['site'].append('Unknown') \n",
    "            \n",
    "            cum_dose = self.cumulative_dose\n",
    "            \n",
    "            for t in target_volumes:\n",
    "                target_properties[p][t] = {} # TODO: use dose information variable instead of results\n",
    "                self.__find_posible_rx_dose(t)\n",
    "                # parse target mask\n",
    "                target_data = self.parse_structure_files(mask_names = t, resolution = 'dose')[t].data\n",
    "                # get the cumulative dose\n",
    "                dose_in_mask = cum_dose[np.where(target_data>0)]\n",
    "                \n",
    "                # find possible rx dose\n",
    "                rx_from_name = self.__find_posible_rx_dose(t)\n",
    "                # find dose statistics\n",
    "                target_properties[p][t]['D95'] = np.percentile(dose_in_mask, 100-max(0, min(100, 95)))\n",
    "                target_properties[p][t]['D98'] = np.percentile(dose_in_mask, 100-max(0, min(100, 98)))\n",
    "                target_properties[p][t]['D2'] = np.percentile(dose_in_mask, 100-max(0, min(100, 2)))\n",
    "                target_properties[p][t]['mean_dose'] = np.mean(dose_in_mask)\n",
    "                target_properties[p][t]['max_dose'] = np.max(dose_in_mask)\n",
    "                \n",
    "        # save target properties as json nicely formatted\n",
    "        with open(os.path.join('temp','data','target_properties.json'), 'w') as f:\n",
    "            json.dump(target_properties, f, indent=4)         \n",
    "        \n",
    "        # save rx dose information as csv\n",
    "        rx_dose_info = pandas.DataFrame(rx_dose_info)\n",
    "        rx_dose_info.to_csv(os.path.join('temp','data','inferred_ rx_dose_info.csv'), index=False)\n",
    "        \n",
    "    def __find_posible_rx_dose(self, string):\n",
    "        string = string.replace(self.user_inputs[\"TYPE_OF_TARGET_VOLUME\"], '')\n",
    "        if 'mm' in string: return 0 # return 0 if the string contains units of mm\n",
    "        pattern = r'\\d+(?:\\.\\d+)?'  # Matches whole numbers and decimal numbers\n",
    "        matches = {float(x):x for x in re.findall(pattern, string)}\n",
    "        return matches[max(matches.keys())] if matches != {} else 0\n",
    "\n",
    "    def store_patient_data_as_hdf5(self, patient_data_directory=None, patient_list=None, mask_resolution = None, output_directory=None):\n",
    "        \"\"\"Transfer data from one or more patients from DICOM-RT formats to HDF5 along with \n",
    "        all of the necessary details like coordinates, location of the isocenter, VSAD, and more. \n",
    "        This allows for a significant increase in I/O speed but increases the hard-drive memory consumption by \n",
    "        creating (in many cases) a large HDF5 file with all of the patient data. Please ensure that enough memory \n",
    "        is available when using this method of data wrangling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        patient_data_directory : str, optional\n",
    "            Location of patient data folders. If None, the location in the `user_inputs.json` file will be used, by default None\n",
    "        patient_list : list, optional\n",
    "            List of patient IDs to transfer into the HDF5 file, by default None\n",
    "        output_directory : str, optional\n",
    "            Folder directory for the output HDf5 file, by default None\n",
    "        \"\"\"\n",
    "\n",
    "        self.mask_resolution = mask_resolution if mask_resolution is not None else 'dose'\n",
    "\n",
    "        if patient_data_directory is not None: self.patient_data_directory = patient_data_directory\n",
    "\n",
    "        if output_directory is None: \n",
    "            output_directory = os.path.join('temp','data', 'patient_data.h5')   \n",
    "\n",
    "        # Create h5file\n",
    "        if self.write_new_hdf5_file:\n",
    "            out_file = h5py.File(output_directory,'w'); out_file.close()\n",
    "\n",
    "        if patient_list is None: patient_list = self.identify_patient_files()\n",
    "\n",
    "        if self.echo_progress: patient_list = tqdm(patient_list, desc=\"Saving HDF5 patient file\", leave=True)\n",
    "        \n",
    "        for p in patient_list:\n",
    "\n",
    "            self.patient_id, files = p, self.run_initial_check(p)\n",
    "            \n",
    "            self.ct = self.parse_ct_study_files(files['ct'])\n",
    "\n",
    "            self.append_data_to_hdf5_file(output_directory, data_type='ct')\n",
    "            \n",
    "            # Parse the dose volime and (optinally) the plan \n",
    "            if 'plan' in files.keys() and files['plan'] != []:\n",
    "                self.dose, self.plan = self.parse_rt_dose_files(files['dose'], files['plan'])\n",
    "            elif 'dose' in files.keys() and files['dose'] != []:\n",
    "                self.dose = self.parse_rt_dose_files(files['dose'])\n",
    "                self.plan = Plan()\n",
    "\n",
    "            self.append_data_to_hdf5_file(output_directory, data_type='dose')\n",
    "\n",
    "            # Parse the contours\n",
    "            for f in files['structures']:\n",
    "            \n",
    "                self.contours = self.parse_structure_files([f], resolution=self.mask_resolution)\n",
    "\n",
    "                self.append_data_to_hdf5_file(output_directory, 'contours')\n",
    "\n",
    "            self.parse_dicom_files(p)\n",
    "\n",
    "        self.__logger.info(f\"The patient data was stored as an HDF5 file in {output_directory}.\")\n",
    "    \n",
    "    def append_data_to_hdf5_file(self, output_directory, data_type):\n",
    "        \"\"\"Append the data for each patient to an HDF5 file for faster I/O\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `output_directory` : str\n",
    "            directory for the folder where the patient data file will be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        if data_type == 'ct':\n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "\n",
    "                # CT data\n",
    "                dset = f.create_dataset(name='/'.join([self.patient_id, 'ct']), data = self.ct.data, compression = self.compression)\n",
    "                dset.attrs['units'] = self.ct.units\n",
    "                dset.attrs['rescale_slope'] = self.ct.rescale_slope\n",
    "                dset.attrs['rescale_intercept'] = self.ct.rescale_intercept\n",
    "                dset.attrs['units'] = self.ct.units\n",
    "                dset.attrs['resolution'] = self.ct.resolution\n",
    "                dset.attrs['x'] = self.ct.coordinates.x\n",
    "                dset.attrs['y'] = self.ct.coordinates.y\n",
    "                dset.attrs['z'] = self.ct.coordinates.z\n",
    "                dset.attrs['dx'] = self.ct.coordinates.dx\n",
    "                dset.attrs['dy'] = self.ct.coordinates.dy\n",
    "                dset.attrs['dz'] = self.ct.coordinates.dz\n",
    "                dset.attrs['image_position'] = self.ct.coordinates.image_position\n",
    "\n",
    "        elif data_type == 'dose':\n",
    "                \n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "                \n",
    "                # Dose data\n",
    "                if self.radiation_type.lower() == 'pronton':\n",
    "                    for b in self.dose.keys():\n",
    "                        dset = f.create_dataset(name='/'.join([self.patient_id, 'dose', str(b)]), data = self.dose[b].data, compression = self.compression )\n",
    "                        dset.attrs['dose_grid_scaling'] = self.dose[b].dose_grid_scaling\n",
    "                        dset.attrs['dose_units'] = self.dose[b].dose_units\n",
    "                        dset.attrs['beam_type'] = self.dose[b].beam_type\n",
    "                        dset.attrs['vsad'] = self.dose[b].vsad\n",
    "                        dset.attrs['number_of_control_points'] = self.dose[b].number_of_control_points\n",
    "                        dset.attrs['gantry_angle'] = self.dose[b].gantry_angle\n",
    "                        dset.attrs['patient_support_angle'] = self.dose[b].patient_support_angle\n",
    "                        dset.attrs['table_top_pitch_angle'] = self.dose[b].table_top_pitch_angle\n",
    "                        dset.attrs['table_top_roll_angle'] = self.dose[b].table_top_roll_angle\n",
    "                        dset.attrs['isocenter'] = self.dose[b].isocenter\n",
    "                        dset.attrs['resolution'] = self.dose[b].resolution\n",
    "                        dset.attrs['x'] = self.dose[b].coordinates.x\n",
    "                        dset.attrs['y'] = self.dose[b].coordinates.y\n",
    "                        dset.attrs['z'] = self.dose[b].coordinates.z\n",
    "                        dset.attrs['dx'] = self.dose[b].coordinates.dx\n",
    "                        dset.attrs['dy'] = self.dose[b].coordinates.dy\n",
    "                        dset.attrs['dz'] = self.dose[b].coordinates.dz\n",
    "                        dset.attrs['image_position'] = self.dose[b].coordinates.image_position\n",
    "                        \n",
    "                #TODO: add values for photon dose\n",
    "                \n",
    "                # Plan data\n",
    "                plan = f.create_group(name=f'{self.patient_id}/plan')\n",
    "                plan.attrs['number_of_beams'] = self.plan.number_of_beams\n",
    "                plan.attrs['plan_label'] = self.plan.plan_label\n",
    "                plan.attrs['patient_sex'] = self.plan.patient_sex\n",
    "                plan.attrs['plan_name'] = self.plan.plan_name\n",
    "\n",
    "        elif data_type == 'contours':\n",
    "            with h5py.File(output_directory,'a') as f:\n",
    "                for c in self.contours.keys():\n",
    "                    name = str(c).replace(\"/\",\"-\")\n",
    "                    dset = f.create_dataset(name='/'.join([self.patient_id, 'contours', self.contours[c].structure_set, str(name)]), \n",
    "                                            data = self.contours[c].data, compression = self.compression )\n",
    "                    dset.attrs['resolution'] = self.contours[c].resolution\n",
    "\n",
    "    def read_data_from_hdf5(self, patient_id = None, contours_type='clinical'):\n",
    "        \"\"\"Read patient data from HDF5 files generated by this class using  the `store_patient_data_as_hdf5` method. \n",
    "        This allows for fast I/O but increases the hard-drive memory consumption by creating (in many cases) a large \n",
    "        HDF5 file with all of the patient data. Please ensure that enough memory is available when using this \n",
    "        method of data wrangling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `patient_id` : str, optional\n",
    "            ID of the patient data to read, by default None\n",
    "        \"\"\"\n",
    "\n",
    "        if patient_id is not None: self.patient_id = patient_id\n",
    "\n",
    "        with h5py.File(self.user_inputs[\"DIRECTORIES\"][\"raw_patient_data\"],'r') as f:\n",
    "\n",
    "            # Plan\n",
    "            plan = f[f'{self.patient_id}/plan']\n",
    "            number_of_beams = plan.attrs['number_of_beams']\n",
    "            patient_sex = plan.attrs['patient_sex']            \n",
    "            plan_label = plan.attrs['plan_label'] \n",
    "            plan_name = plan.attrs['plan_name']\n",
    "            modality = plan.attrs['modality']\n",
    "            self.plan = Plan(number_of_beams, patient_sex, plan_label, plan_name, modality)\n",
    "\n",
    "            # CT\n",
    "            params = [f[f'{self.patient_id}/ct'][...].shape]\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['resolution'])\n",
    "            params.append(f[f'{self.patient_id}/ct'][...].max())\n",
    "            params.append(f[f'{self.patient_id}/ct'][...].min())\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['units'])\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['rescale_slope'])\n",
    "            params.append(f[f'{self.patient_id}/ct'].attrs['rescale_intercept'])\n",
    "            params.append(f[f'{self.patient_id}/ct'][...])\n",
    "            x, y, z = f[f'{self.patient_id}/ct'].attrs['x'], f[f'{self.patient_id}/ct'].attrs['y'], f[f'{self.patient_id}/ct'].attrs['z']\n",
    "            dx, dy, dz = f[f'{self.patient_id}/ct'].attrs['dx'], f[f'{self.patient_id}/ct'].attrs['dy'], f[f'{self.patient_id}/ct'].attrs['dz']\n",
    "            image_position = f[f'{self.patient_id}/ct'].attrs['image_position']\n",
    "            self.original_ct_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "            params.append(self.original_ct_coordinates)\n",
    "            self.ct = CT(*params)\n",
    "\n",
    "            # Dose\n",
    "            self.dose = {int(bn):None for bn in f[f'{self.patient_id}/dose'].keys()}\n",
    "            for k in self.dose.keys():\n",
    "                params = [f[f'{self.patient_id}/dose/{k}'][...].shape]\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...].max())\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...].min())\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['resolution'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['dose_grid_scaling'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['dose_units'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'][...])\n",
    "                x, y, z = f[f'{self.patient_id}/dose/{k}'].attrs['x'], f[f'{self.patient_id}/dose/{k}'].attrs['y'], f[f'{self.patient_id}/dose/{k}'].attrs['z']\n",
    "                dx, dy, dz = f[f'{self.patient_id}/dose/{k}'].attrs['dx'], f[f'{self.patient_id}/dose/{k}'].attrs['dy'], f[f'{self.patient_id}/dose/{k}'].attrs['dz']\n",
    "                image_position = f[f'{self.patient_id}/dose/{k}'].attrs['image_position']\n",
    "                self.original_dose_coordinates = Coordinates(x,y,z,dx,dy,dz,image_position)\n",
    "                params.append(self.original_dose_coordinates)\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['modality'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['beam_type'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['vsad'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['number_of_control_points'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['gantry_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['patient_support_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['table_top_pitch_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['table_top_roll_angle'])\n",
    "                params.append(f[f'{self.patient_id}/dose/{k}'].attrs['isocenter'])\n",
    "                self.dose[k] = Dose(*params)\n",
    "                \n",
    "            #TODO: differentiate between photon and proton dose\n",
    "\n",
    "            # Contours\n",
    "            self.contours = {c:None for c in  f[f'{self.patient_id}/contours/{contours_type}'].keys()}\n",
    "            for k in self.contours.keys():\n",
    "                data = f[f'{self.patient_id}/contours/{contours_type}/{k}'][...]\n",
    "                resolution = f[f'{self.patient_id}/contours/{contours_type}/{k}'].attrs['resolution']\n",
    "                if resolution == 'ct':\n",
    "                    coordinates = self.original_ct_coordinates\n",
    "                else:\n",
    "                    coordinates = self.original_dose_coordinates\n",
    "                self.contours[k] = Mask(data, resolution, coordinates)\n",
    "            # Grab CTVs in case the desired contours are automatic\n",
    "            if contours_type == 'auto':\n",
    "                for k in [c for c in f[f'{self.patient_id}/contours/clinical'].keys() if 'ctv' in c and 'fsctv' not in c and 'pctv' not in c]:\n",
    "                    data = f[f'{self.patient_id}/contours/clinical/{k}'][...]\n",
    "                    resolution = f[f'{self.patient_id}/contours/clinical/{k}'].attrs['resolution']\n",
    "                    coordinates = self.original_ct_coordinates if resolution == 'ct' else self.original_dose_coordinates\n",
    "                    self.contours[k] = Mask(data, resolution, coordinates)\n",
    "\n",
    "    @property\n",
    "    def cumulative_dose(self):\n",
    "        \"\"\"Compute the cumulative dose volume for all of the beams in the plan.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            3D array containing the cumulative dose volume.\n",
    "        \"\"\"\n",
    "        return np.sum([self.dose[b].data for b in self.dose.keys()], axis = 0)\n",
    "\n",
    "    @property\n",
    "    def identify_parallel_capabilities(self):\n",
    "        \n",
    "        self.n_threads = self.user_inputs['PARALLELIZATION']['number_of_processors'] \n",
    "        if self.n_threads is None: self.n_threads = mp.cpu_count()//2 # assumes that half of the CPUs are available for parallel processing\n",
    "        if self.n_threads >  mp.cpu_count():\n",
    "            self.__logger.warning(f\"The number of threads ({self.n_threads}) exceeds the number of available CPUs ({mp.cpu_count()}).\")\n",
    "            self.n_threads = mp.cpu_count()//2\n",
    "            self.__logger.warning(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        self.parallelize = True if self.user_inputs['PARALLELIZATION']['parallelize_data_preprocessing'] and self.n_threads > 1 else False   \n",
    "        \n",
    "        if not self.parallelize:\n",
    "            self.n_threads = 1\n",
    "            self.__logger.info(\"Preprocessing will be performed sequentially. Consider using parallel processing to speed up the process.\")\n",
    "            return\n",
    "\n",
    "        # log the available resources\n",
    "        self.__logger.info(f\"Number of CPUs (Virtual): {mp.cpu_count()}\")\n",
    "        \n",
    "        # get the number of threads ready to perform work concurrently\n",
    "        if self.user_inputs['PARALLELIZATION']['number_of_processors'] is None and self.parallelize:\n",
    "            self.n_threads = mp.cpu_count()//2 # assumes that half of the CPUs are available for parallel processing\n",
    "            self.__logger.info(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        else:\n",
    "            self.n_threads = int(self.user_inputs['PARALLELIZATION']['number_of_processors'])\n",
    "            if self.parallelize and self.n_threads > mp.cpu_count():\n",
    "                self.__logger.warning(f\"The number of threads ({self.n_threads}) exceeds the number of available CPUs ({mp.cpu_count()}).\")\n",
    "                self.__logger.warning(f\"Parallel processing will be used with {mp.cpu_count()} threads\")\n",
    "                self.n_threads = mp.cpu_count()\n",
    "            elif self.n_threads == 1:\n",
    "                self.parallelize = False\n",
    "                self.__logger.info(\"Parallel processing will not be used since only 1 thread was requested\")\n",
    "        \n",
    "        if self.n_threads > 1 and self.parallelize:\n",
    "            self.__logger.info(f\"Parallel processing will be used with {self.n_threads} threads\")\n",
    "        else:       \n",
    "            self.__logger.info(\"Preprocessing will be performed sequentially. Consider using parallel processing to speed up the process.\")\n",
    "            \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
